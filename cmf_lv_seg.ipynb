{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Set-up and some packages for manipulation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "204f84f618ce4880"
  },
  {
   "cell_type": "code",
   "id": "b58a888b09702164",
   "metadata": {},
   "source": [
    "import unittest\n",
    "import random\n",
    "import copy as cp\n",
    "\n",
    "import nrrd\n",
    "import numpy as np\n",
    "import astra\n",
    "import scipy.spatial\n",
    "from scipy.spatial.distance import cdist\n",
    "import torchio\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as torch_transform\n",
    "\n",
    "from pykeops.torch import Vi, Vj\n",
    "from pykeops.torch import LazyTensor\n",
    "\n",
    "from skimage import measure\n",
    "from skimage.util import random_noise\n",
    "import point_cloud_utils as pcu\n",
    "\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import mcubes\n",
    "from simpleicp import PointCloud, SimpleICP\n",
    "\n",
    "from src.algs.arm import lv_indicator\n",
    "from src.tools.cmf.cmf import CMF_3D\n",
    "from src.tools.recon.projector import forward_projector, backward_projector\n",
    "from src.tools.manip.manip import normalize_volume\n",
    "\n",
    "# data fetching and handling\n",
    "from data.check_database import load_remote_data\n",
    "from data.fetch_data import fetch_data\n",
    "from src.tools.data.loadvolumes import LoadVolumes"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data types and set-up for loading the data from a remote html server"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb1fb73ab6fa8e58"
  },
  {
   "cell_type": "code",
   "id": "d9c40df21b1d5def",
   "metadata": {},
   "source": [
    "lv_model_volume = None\n",
    "lv_model_frames = None\n",
    "lv_motion_frames = None\n",
    "lv_volume = None\n",
    "\n",
    "resolution = 64\n",
    "\n",
    "volume = np.zeros([resolution, resolution, resolution])\n",
    "params = dict(a=1, c=2, sigma=-1)\n",
    "transform_params = [np.eye(3, 3), [16, 16, 0], 1.5]\n",
    "\n",
    "recon_mode = 'basic'\n",
    "fprojector = forward_projector(recon_mode)\n",
    "\n",
    "lv_model_volume = lv_indicator(volume, params, transform_params)\n",
    "lv_model_frames = fprojector(lv_model_volume)\n",
    "lv_motion_frames = np.zeros(lv_model_frames.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Individual patient fetching from remote server"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38a2e92578164b7a"
  },
  {
   "cell_type": "code",
   "id": "5383cfd2bd9ca88b",
   "metadata": {},
   "source": [
    "dicom_loader = LoadVolumes()\n",
    "\n",
    "# initialize data fetching from remote, configuration is in data/remote.yml\n",
    "data_loaded = False\n",
    "url, datasets = load_remote_data()\n",
    "\n",
    "# fetch specific patient data\n",
    "dicom_name = datasets['raw/']['turkey_par/'][10]\n",
    "data_url = url + '/raw/' + 'turkey_par/' + dicom_name\n",
    "\n",
    "# fetch the data from remote\n",
    "data = fetch_data(data_url)\n",
    "\n",
    "# load data with the dicom loader\n",
    "frames, data_loaded = dicom_loader.LoadSinglePatient(data)\n",
    "\n",
    "# normalizing the frame values\n",
    "normalize_volume(frames)\n",
    "frames = frames + 1\n",
    "\n",
    "assert (data_loaded)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Series of patient fetching from remote server"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3723fcbb303c1dc1"
  },
  {
   "cell_type": "code",
   "source": [
    "dicom_loader = LoadVolumes()\n",
    "\n",
    "# initialize data fetching from remote, configuration is in data/remote.yml\n",
    "data_loaded = False\n",
    "url, datasets = load_remote_data()\n",
    "\n",
    "# read all filenames from the url\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "page = requests.get(url + '/recon/' + 'spie_2024/' + 'misc/' + 'label/')\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "label_names = []\n",
    "for label_ref in soup.find_all('a'):\n",
    "    label_names.append(label_ref.get('href'))\n",
    "\n",
    "page = requests.get(url + '/recon/' + 'spie_2024/' + 'misc/' + 'data/')\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "data_names = []\n",
    "for label_ref in soup.find_all('a'):\n",
    "    data_names.append(label_ref.get('href'))\n",
    "\n",
    "subjects = []\n",
    "subjects_data = []\n",
    "\n",
    "# fetch specific patient data\n",
    "for index in range(len(data_names)):\n",
    "\n",
    "    dicom_name = data_names[index]\n",
    "    label_name = label_names[index]\n",
    "    data_url = url + '/recon/' + 'spie_2024/' + 'misc/' + 'data/' + dicom_name\n",
    "    label_url = url + '/recon/' + 'spie_2024/' + 'misc/' + 'label/' + label_name\n",
    "    \n",
    "    # fetch the data from remote\n",
    "    data = fetch_data(data_url)\n",
    "    lab = fetch_data(label_url)\n",
    "    \n",
    "    # load data with the dicom loader\n",
    "    volume, data_loaded = dicom_loader.LoadSinglePatient(data)\n",
    "    header = nrrd.read_header(lab)\n",
    "    labels = nrrd.read_data(header, lab)\n",
    "    \n",
    "    # looks like the label export is a bit tricky so loading shall be updated\n",
    "    prob_val_1 = np.sum(np.where(np.transpose(labels, [2, 1, 0]) == 1, 1, 0))\n",
    "    prob_val_2 = np.sum(np.where(np.transpose(labels, [2, 1, 0]) == 2, 1, 0))\n",
    "    \n",
    "    if prob_val_1 > prob_val_2:\n",
    "        labels = np.where(np.transpose(labels, [2, 1, 0]) == 2, 1, 0)\n",
    "    else:\n",
    "        labels = np.where(np.transpose(labels, [2, 1, 0]) == 1, 1, 0)\n",
    "        \n",
    "    subject = {\n",
    "        'spect' : volume,\n",
    "        'left_ventricle' : labels\n",
    "    }\n",
    "    subjects.append(subject)\n",
    "    \n",
    "    age, gender, weight, height = dicom_loader.CalculatePatientStatistics()\n",
    "    subject_data = {\n",
    "        'age' : age,\n",
    "        'gender' : gender,\n",
    "        'weight' : weight,\n",
    "        'height' : height\n",
    "    }\n",
    "    subjects_data.append(subject_data)\n",
    "\n",
    "    print(\"Volume shape: \", volume.shape, \"Labels shape:\", labels.shape)\n",
    "\n",
    "    # normalizing the frame values\n",
    "    normalize_volume(volume)\n",
    "\n",
    "assert (data_loaded)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e931eb66fd894f6a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(len(subjects))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be7ea13a1704646",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def compute_dataset_means(subjects_data, indices = None):\n",
    "    ages = np.zeros(0)\n",
    "    weights = np.zeros(0)\n",
    "    heights = np.zeros(0)\n",
    "    \n",
    "    num_male = 0\n",
    "    num_female = 0\n",
    "    \n",
    "    # cleaning data\n",
    "    for i in indices:\n",
    "        age = subjects_data[i]['age'] \n",
    "        if age > 0:\n",
    "            ages = np.append(ages, age)\n",
    "        \n",
    "        weight = subjects_data[i]['weight']\n",
    "        if weight > 0:\n",
    "            weights = np.append(weights, weight)\n",
    "        \n",
    "        height = subjects_data[i]['height']\n",
    "        if height > 0:\n",
    "            heights = np.append(heights, height)\n",
    "        \n",
    "        if subjects_data[i]['gender'] == 'F':\n",
    "            num_female += 1\n",
    "        elif subjects_data[i]['gender'] == 'M':\n",
    "            num_male += 1\n",
    "\n",
    "    return num_female, num_male, ages, heights, weights        "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "85283036b0dd2834",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "num_female, num_male, ages, heights, weights = compute_dataset_means(subjects_data)\n",
    "\n",
    "print(\"Number of females: \", num_female, \"Number of males: \", num_male)\n",
    "print(\"Average age: \", np.mean(ages))\n",
    "print(\"Average height: \", np.mean(heights))\n",
    "print(\"Average weight: \", np.mean(weights))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a0b51caa19a3557",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "dicom_loader = LoadVolumes()\n",
    "\n",
    "# initialize data fetching from remote, configuration is in data/remote.yml\n",
    "data_loaded = False\n",
    "url, datasets = load_remote_data()\n",
    "\n",
    "# read all filenames from the url\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "page = requests.get(url + '/recon/' + 'spie_2024/' + 'bela/' + 'label/')\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "label_names = []\n",
    "for label_ref in soup.find_all('a'):\n",
    "    label_names.append(label_ref.get('href'))\n",
    "\n",
    "page = requests.get(url + '/recon/' + 'spie_2024/' + 'bela/' + 'data/')\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "data_names = []\n",
    "for label_ref in soup.find_all('a'):\n",
    "    data_names.append(label_ref.get('href'))\n",
    "\n",
    "subjects_bela = []\n",
    "subjects_bela_data = []\n",
    "\n",
    "# fetch specific patient data\n",
    "for index in range(len(data_names)):\n",
    "\n",
    "    dicom_name = data_names[index]\n",
    "    label_name = label_names[index]\n",
    "    data_url = url + '/recon/' + 'spie_2024/' + 'bela/' + 'data/' + dicom_name\n",
    "    label_url = url + '/recon/' + 'spie_2024/' + 'bela/' + 'label/' + label_name\n",
    "    \n",
    "    # fetch the data from remote\n",
    "    data = fetch_data(data_url)\n",
    "    lab = fetch_data(label_url)\n",
    "    \n",
    "    # load data with the dicom loader\n",
    "    volume, data_loaded = dicom_loader.LoadSinglePatient(data)\n",
    "    header = nrrd.read_header(lab)\n",
    "    labels = nrrd.read_data(header, lab)\n",
    "    \n",
    "    # looks like the label export is a bit tricky so loading shall be updated\n",
    "    prob_val_1 = np.sum(np.where(np.transpose(labels, [2, 1, 0]) == 1, 1, 0))\n",
    "    prob_val_2 = np.sum(np.where(np.transpose(labels, [2, 1, 0]) == 2, 1, 0))\n",
    "    \n",
    "    if prob_val_1 > prob_val_2:\n",
    "        labels = np.where(np.transpose(labels, [2, 1, 0]) == 2, 1, 0)\n",
    "    else:\n",
    "        labels = np.where(np.transpose(labels, [2, 1, 0]) == 1, 1, 0)\n",
    "    \n",
    "    subject = {\n",
    "        'spect' : volume,\n",
    "        'left_ventricle' : labels\n",
    "    }\n",
    "    subjects_bela.append(subject)\n",
    "    \n",
    "    age, gender, weight, height = dicom_loader.CalculatePatientStatistics()\n",
    "    subject_data = {\n",
    "        'age' : age,\n",
    "        'gender' : gender,\n",
    "        'weight' : weight,\n",
    "        'height' : height\n",
    "    }\n",
    "    subjects_bela_data.append(subject_data)\n",
    "\n",
    "    print(\"Volume shape: \", volume.shape, \"Labels shape:\", labels.shape)\n",
    "\n",
    "    # normalizing the frame values\n",
    "    normalize_volume(volume)\n",
    "\n",
    "assert (data_loaded)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "832dfde292ea173b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(len(subjects_bela))\n",
    "print(len(subjects_bela_data))\n",
    "\n",
    "num_female, num_male, ages, heights, weights = compute_dataset_means(subjects_bela_data, [0, 1, 2, 3, 4, -9, -4, -3, -2, -1]) # MPH : list(range(15, 25)), Par: [-5, -6, -7, -7], cardiod list(range(4, 15)), cardioc [0, 1, 2, 3, 4, -9, -4, -3, -2, -1]\n",
    "\n",
    "print(\"Number of females: \", num_female, \"Number of males: \", num_male)\n",
    "print(\"Average age: \", np.mean(ages))\n",
    "print(\"Average height: \", np.mean(heights))\n",
    "print(\"Average weight: \", np.mean(weights))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c0e33e25fcbf5a59",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generating shape priors with the proposed cardiac model, the size proportional to the frames that are fetched already"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e78053e58a93fec"
  },
  {
   "cell_type": "code",
   "id": "3273a29344ff3c6d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "num_frames, width, height = frames.shape\n",
    "\n",
    "bprojectpor = backward_projector()\n",
    "lv_volume = bprojectpor(frames)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# from mpl_toolkits import mplot3d\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib notebook\n",
    "# \n",
    "# # turkey_par, 2 (index: 20, 20:40, 40:60), 4(index: 32, 20:40, 30:50), 7(index: 25, 25:45, 40:60), 10(index: 32, 20:40, 30:50)\n",
    "# index = 32\n",
    "# \n",
    "# fig = plt.figure()\n",
    "# fig.set_size_inches((1, 1))\n",
    "# ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "# ax.set_axis_off()\n",
    "# fig.add_axes(ax)\n",
    "#     \n",
    "# plt.imshow(lv_volume[index, 20:40, 30:50], aspect='equal')\n",
    "# plt.savefig(\"/home/jackson/GIT/ELTE/papers/left_ventricle_segmentation/allerton_2023/images/poster/\" + \"patient_10\" + \".png\", bbox_inches='tight', pad_inches=0)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "644f90f5adca5626",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "lv_volume = np.random.rand(64, 64, 64)\n",
    "normalize_volume(lv_volume)\n",
    "\n",
    "num_prior = 9\n",
    "shape_priors = np.zeros([num_prior, *lv_volume.shape])\n",
    "\n",
    "wall_thickness = np.random.uniform(0.3, 1.0, num_prior)\n",
    "rot_angles = np.random.uniform(0, 2 * np.pi, num_prior)\n",
    "curvature = np.random.uniform(1.5, 3, num_prior)\n",
    "sigmas = np.random.uniform(-0.5, -1, num_prior)\n",
    "\n",
    "for i in range(num_prior):\n",
    "    volume = np.zeros([*lv_volume.shape])\n",
    "    params = dict(a=wall_thickness[i], c=curvature[i], sigma=sigmas[i])\n",
    "    rot_mx = R.from_quat([0, 0, np.sin(rot_angles[i]), np.cos(rot_angles[i])])\n",
    "\n",
    "    transform_params = [np.eye(3, 3), [16, 16, 0], 1.5]\n",
    "    shape_priors[i] = lv_indicator(volume, params, transform_params, a_plot=False)\n",
    "\n",
    "    recon_mode = 'basic'\n",
    "    fprojector = forward_projector(recon_mode)\n",
    "\n",
    "    frames = fprojector(shape_priors[i])\n",
    "    \n",
    "# lv_volume = shape_priors[3]"
   ],
   "metadata": {},
   "id": "903b31007bf0a73f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Implementation of the functions being tested, first their packages loaded"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c0d8a2e68608534"
  },
  {
   "cell_type": "code",
   "id": "e773c60a5970be8",
   "metadata": {},
   "source": [
    "def GaussKernel(sigma):\n",
    "    x, y, b = Vi(0, 2), Vj(1, 2), Vj(2, 2)\n",
    "    gamma = 1 / (2 * sigma * sigma)\n",
    "    D2 = x.sqdist(y) / (2 * 64 * 64)\n",
    "    K = (-D2 * gamma).exp()\n",
    "    return ((0.3989 / sigma) * K * b).sum_reduction(axis=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "80991d6459e1cceb",
   "metadata": {},
   "source": [
    "def GaussKernel(x, y, sigma):\n",
    "    gamma = 1 / (2 * sigma * sigma)\n",
    "\n",
    "    if len(x.shape) > 3 or len(y.shape) > 3:\n",
    "        D2 = torch.zeros((x.shape[0], y.shape[0], *x.shape[1:]))\n",
    "        for i in range(x.shape[0]):\n",
    "            for j in range(y.shape[0]):\n",
    "                D2[i, j] = torch.abs(x[i] - y[j]) ** 2 / (2 * 64 * 64)\n",
    "\n",
    "        K = (-D2 * gamma).exp()\n",
    "        return torch.sum((0.3989 / sigma) * K, dim=(2, 3, 4))\n",
    "    else:\n",
    "        D2 = torch.abs(x - y) ** 2 / (2 * 64 * 64)\n",
    "        K = (-D2 * gamma).exp()\n",
    "        return torch.sum((0.3989 / sigma) * K)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "aad3ff29d15f1264",
   "metadata": {},
   "source": [
    "# orthogonal complement calculation based on https://github.com/statsmodels/statsmodels/issues/3039 \n",
    "def orthogonal_complement(x, normalize=False, threshold=1e-15):\n",
    "    \"\"\"Compute orthogonal complement of a matrix\n",
    "\n",
    "    this works along axis zero, i.e. rank == column rank,\n",
    "    or number of rows > column rank\n",
    "    otherwise orthogonal complement is empty\n",
    "\n",
    "    TODO possibly: use normalize='top' or 'bottom'\n",
    "\n",
    "    \"\"\"\n",
    "    r, c = x.shape\n",
    "    if r < c:\n",
    "        import warnings\n",
    "        warnings.warn('fewer rows than columns', UserWarning)\n",
    "\n",
    "    # we assume svd is ordered by decreasing singular value, o.w. need sort\n",
    "    s, v, d = torch.linalg.svd(x)\n",
    "    rank = torch.sum(torch.where(torch.diag(v) > threshold, 1.0, 0.0))\n",
    "\n",
    "    oc = s[:, rank:]\n",
    "\n",
    "    if normalize:\n",
    "        k_oc = oc.shape[1]\n",
    "        oc = oc.dot(torch.linalg.inv(oc[:k_oc, :]))\n",
    "    return oc"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "raw",
   "id": "f619796847a890f3",
   "metadata": {},
   "source": [
    "Will need to derivate the Procrustes alignment procedure to the mean shape and try the Gromov-Wasserstein distance too"
   ]
  },
  {
   "cell_type": "code",
   "id": "de9a762e26743c48",
   "metadata": {},
   "source": [
    "x1 = np.random.rand(1000, 3)\n",
    "x2 = np.random.rand(1000, 3)\n",
    "\n",
    "M = x1.T @ x2\n",
    "\n",
    "U, S, V = np.linalg.svd(M)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c5ed8297f191ce6b",
   "metadata": {},
   "source": [
    "proj = V @ U.T"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a8657019e261c564",
   "metadata": {},
   "source": [
    "proj.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8a35b8380b20370b",
   "metadata": {},
   "source": [
    "proj"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "53c6835bad41994d",
   "metadata": {},
   "source": [
    "proj_diff = np.linalg.norm(x1 - x2 @ proj)\n",
    "diff = np.linalg.norm(x1 - x2)\n",
    "\n",
    "print(proj_diff)\n",
    "print(diff)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "69c65b11ef19942f",
   "metadata": {},
   "source": [
    "from scipy.spatial import procrustes\n",
    "nx1, nx2,  M = procrustes(x1, x2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create the shape distribution here"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "269530d80bc97cab"
  },
  {
   "cell_type": "code",
   "id": "e05e32f0adcbcc12",
   "metadata": {},
   "source": [
    "shape_priors.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7a2700eb3396a3f2",
   "metadata": {},
   "source": [
    "num_samples = shape_priors.shape[0]\n",
    "\n",
    "num_verts = 100\n",
    "rand_positions = torch.zeros((shape_priors.shape[0], num_verts, 3))\n",
    "\n",
    "for i in range(shape_priors.shape[0]):\n",
    "    verts, faces, normals, values = measure.marching_cubes(shape_priors[i], 0)\n",
    "    fid, bc = pcu.sample_mesh_random(verts, faces, num_verts)\n",
    "    rand_positions[i] = torch.from_numpy(pcu.interpolate_barycentric_coords(faces, fid, bc, verts))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cadcc9a8b972660a",
   "metadata": {},
   "source": [
    "rand_positions.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "90d505d9db992019",
   "metadata": {},
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "# get random lv prior surface points\n",
    "lv_surf = rand_positions[3]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter(lv_surf[:, 0], lv_surf[:, 1], lv_surf[:, 2])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "46bcf280e871e959",
   "metadata": {},
   "source": [
    "z_i = rand_positions  # generated random positions on the lv surface\n",
    "z_i_a = torch.zeros(rand_positions.shape)  # aligned shapes\n",
    "\n",
    "# compute the scaled and mean shapes of each training sample\n",
    "z_i_centroid = torch.mean(z_i, dim=1)\n",
    "z_i_size = torch.linalg.norm(z_i - z_i_centroid[:, None, :], dim=(1, 2))\n",
    "z_i_s = z_i / z_i_size[:, None, None]\n",
    "\n",
    "# do the algorithm 3.2 in [1], meaning that aligning each shape to the first one\n",
    "prev_mean_shape = z_i_s[0]\n",
    "mean_shape = torch.zeros(*prev_mean_shape.shape)\n",
    "iteration = 0\n",
    "while torch.linalg.norm(prev_mean_shape - mean_shape) >= 1e-2 and iteration < 10:\n",
    "    for i in range(1, z_i.shape[0]):\n",
    "        M = prev_mean_shape.t() @ z_i_s[i] # do the Procrustes analysis on the selected mean and current\n",
    "        U, D, V = torch.linalg.svd(M)\n",
    "        proj_rot_ref = V @ U.t()\n",
    "        \n",
    "        z_i_a[i] = z_i_s[i] @ proj_rot_ref\n",
    "    \n",
    "    prev_mean_shape = mean_shape\n",
    "    mean_shape = torch.mean(z_i_a, dim=0)\n",
    "    iteration += 1  # it is advised in [1] that only two iterations suffice"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a46cc387f81799a8",
   "metadata": {},
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "index = 2\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter(z_i_a[index, :, 0], z_i_a[index, :, 1], z_i_a[index, :, 2])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "raw",
   "id": "d385377bb9757c52",
   "metadata": {},
   "source": [
    "Now try the differentiated gradient version of the previous method based on [2]\n",
    "Apprach 1: Adaptation of https://colab.research.google.com/drive/15yuLZ5uwBwjl8U3zUVFDh35PYnxaaNbk and https://towardsdatascience.com/step-by-step-backpropagation-through-singular-value-decomposition-with-code-in-tensorflow-8056f7fbcbf3"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Gradient/differential of the PCA in kernel space, orthogonal complement calculation as well"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7345af22a41f515b"
  },
  {
   "cell_type": "code",
   "id": "1f364a5df9d4a78e",
   "metadata": {},
   "source": [
    "X1 = z_i[0]\n",
    "X2 = z_i[1]\n",
    "print(X1.shape)\n",
    "\n",
    "U, D, V_t = torch.linalg.svd(X1.t() @ X2)\n",
    "V = V_t.t()\n",
    "print(\" U: \", U.shape, \" D: \", torch.diag(D).shape, \" V_t: \", V_t.shape)\n",
    "R_cap = U @ V_t\n",
    "Beta_cap = torch.trace(torch.diag(D)) / torch.linalg.norm(X1) ** 2\n",
    "\n",
    "print(torch.linalg.norm(X2 - Beta_cap * X1 @ R_cap))\n",
    "\n",
    "X1_a = z_i_a[0]\n",
    "X2_a = z_i_a[1]\n",
    "print(torch.linalg.norm(X2_a - X1_a))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "12fa25c833278461",
   "metadata": {},
   "source": [
    "# Testing out the derivatives dimensions, Approach 1. using SVD..."
   ]
  },
  {
   "cell_type": "code",
   "id": "b5afa44e6102a77d",
   "metadata": {},
   "source": [
    "dU = (torch.eye(U.shape[0]) - U @ U.t()) @ V\n",
    "dV = (torch.eye(V.shape[0]) - V @ V.t()) @ U\n",
    "print(dU.shape)\n",
    "print(dV.shape)\n",
    "\n",
    "dR = dU @ V_t + U @ dV\n",
    "print(dR)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Approach 2. for derivative using Rodrigues formula resulting in a 3-rank tensor "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61fe93bf06d47758"
  },
  {
   "cell_type": "code",
   "id": "c4ba79ee5e516427",
   "metadata": {},
   "source": [
    "# will need optimization on this tridiag tensor\n",
    "ssc = lambda v: torch.tensor([[0, -v[2], v[1]], [v[2], 0, -v[0]], [-v[1], v[0], 0]])\n",
    "RU = lambda A, B: torch.eye(3) + ssc(torch.cross(A, B)) + ssc(torch.cross(A, B))**2 * (1-torch.dot(A,B)) / torch.norm(torch.cross(A,B)) ** 2\n",
    "\n",
    "# derivative respect to z_c first\n",
    "dRU = lambda A, B: ssc(A) + 2 * ssc(torch.cross(A, B)) + ssc(A) + ssc(torch.cross(A, B)) ** 2 * (1 / (1 + torch.dot(A, B) ** 2)) * A \n",
    "\n",
    "N = z_i.shape[1]  # number of elements\n",
    "M_til = torch.zeros([3 * N, 3 * N, 3 * N])\n",
    "\n",
    "id = torch.eye(3)\n",
    "for j in range(0, 3 * N, 1):\n",
    "    e_j = id[j % 3]\n",
    "    for i in range(0, N, 3):\n",
    "        M_til[i:i+3, i:i+3, j] = dRU(z_i[0, i] * e_j, z_i[5, i])\n",
    "\n",
    "print(M_til.shape)\n",
    "# from mpl_toolkits import mplot3d\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib notebook\n",
    "# plt.imshow(cross)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def zero_volume_boundary(a_volume, a_width):\n",
    "\n",
    "    a_volume[:a_width, :, :] = 0\n",
    "    a_volume[-a_width:, :, :] = 0\n",
    "    a_volume[:, :a_width, :] = 0\n",
    "    a_volume[:, -a_width:, :] = 0\n",
    "    a_volume[:, :, :a_width] = 0\n",
    "    a_volume[:, :, -a_width:] = 0"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4d7a680e5d48c92",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def flip_vals(A,val1,val2):\n",
    "\n",
    "    # Find the difference between two values\n",
    "    diff = val2 - val1\n",
    "\n",
    "    # Scale masked portion of A based upon the difference value in positive \n",
    "    # and negative directions and add up with A to have the desired output\n",
    "    return A + diff*(A==val1) - diff*(A==val2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64b5399a7a8a6643",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def nonlinear_shape_prior(shape_priors, kernel, sigma, centering_point):\n",
    "    \"\"\"\n",
    "    Nonlinear statistics shape prior based on kernel density estimation in the feature space\n",
    "        [1] Shape statistics in kernel space for variational image segmentation - Daniel Cremers, Timo Kohlberger,\n",
    "                                                                                  Christoph Schnoerr\n",
    "        [2] Active Shape Models - Their Training and Application - T. F. Cootes, C. J. Taylor, D. H. Cooper, J. Graham\n",
    "\n",
    "    Args:\n",
    "        z:\n",
    "        z_i:\n",
    "        sigma:\n",
    "\n",
    "    Returns:\n",
    "        energy:\n",
    "    \"\"\"\n",
    "    m = shape_priors.shape[0]\n",
    "    \n",
    "    E = (1 / m) * torch.ones([m, m], dtype=torch.float64)\n",
    "    K = torch.zeros([m, m], dtype=torch.float64)\n",
    "    \n",
    "    height, width, depth = shape_priors[0].shape\n",
    "    z_i = []\n",
    "    shape_face_count = torch.zeros([m], dtype=torch.int32)\n",
    "    shape_faces = []\n",
    "    for i in range(m):\n",
    "        verts_shape, tri_shape = mcubes.marching_cubes(shape_priors[i], 0.0)\n",
    "        cur_prior_shape = verts_shape / depth\n",
    "        \n",
    "        # set mesh size to 1 and move it to the centering point\n",
    "        verts_dist = cdist(cur_prior_shape, cur_prior_shape, 'euclidean')\n",
    "        verts_scaled = cur_prior_shape * 1.0 / verts_dist.max()\n",
    "        verts_scaled_translation = centering_point - verts_scaled.mean(axis=0)\n",
    "        verts_translated = verts_scaled + verts_scaled_translation \n",
    "\n",
    "        z_i.append(torch.from_numpy(verts_translated))\n",
    "        shape_faces.append(tri_shape)\n",
    "        shape_face_count[i] = tri_shape.shape[0]\n",
    "     \n",
    "    min_shape_face_count = shape_face_count.min()\n",
    "    # if k_til is wrongfully implemented or slow, or numerically unstable, \n",
    "    # then one can use K_til̃ = K − KE − EK + EKE\n",
    "    mean_shape = z_i[ int(m / 2) ] # try it with Wasserstein barycenter here compute the mean shape\n",
    "    mean_shape_face = shape_faces[ int(m / 2) ] # save the faces as well\n",
    "    \n",
    "    \n",
    "    for i in range(m):\n",
    "        for j in range(m):\n",
    "            K[i, j] = kernel(z_i[i], z_i[j], sigma)\n",
    "    \n",
    "    K_til= K - K @ E - E @ K + E @ K @ E\n",
    "    \n",
    "    # keep only real eigenvalues and eigenvectors\n",
    "    L, V = torch.linalg.eigh(K_til)\n",
    "    L = torch.flip(L, [0])\n",
    "    V = torch.fliplr(V)\n",
    "    \n",
    "    limit_val = 1e-6\n",
    "    if (L <= limit_val).any():\n",
    "        first_cplx = torch.where(L <= limit_val)[0][0]\n",
    "        sigma_ort = L[first_cplx - 1] / 2.0\n",
    "        \n",
    "        L[first_cplx:] = 0.0\n",
    "        V[:, first_cplx:] = 0.0\n",
    "        reg_mx = torch.eye(K.shape[0])\n",
    "        \n",
    "        Sigma_ort = V @ torch.diag(L) @ V.t() + sigma_ort * (reg_mx - V @ V.t())\n",
    "    else:  # bad bad things happen\n",
    "        first_cplx = -1\n",
    "        sigma_ort = 1\n",
    "        Sigma_ort = V @ torch.diag(L) @ V.t()\n",
    "    \n",
    "    return z_i, torch.linalg.inv(Sigma_ort), L, V, sigma_ort, sigma, first_cplx, min_shape_face_count, mean_shape, mean_shape_face, K.sum(), K"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c8af74f62e26ef85",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def k_til(k, sigma, x, y, z_i, m):\n",
    "    sum = 0\n",
    "    for i in range(m):\n",
    "        sum -= (1 / m) * (k(x, z_i[i], sigma) + k(y, z_i[i], sigma))\n",
    "\n",
    "    sum += k(x, y, sigma)\n",
    "\n",
    "    for i in range(m):\n",
    "        for j in range(m):\n",
    "                sum += (1 / (m ** 2)) * k(z_i[i], z_i[j], sigma)\n",
    "\n",
    "    return sum"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4270ad9704fefc9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def E_phi(V, kernel, sigma, z_i, z, L, L_ort, r, m):\n",
    "    loss = 0.0\n",
    "    # takes forever to compute it this way\n",
    "    for k in range(r):\n",
    "        for i in range(m):\n",
    "            loss += V[k, i] * (k_til(kernel, sigma, z_i[i], z, z_i, m) ** 2) * (L[k] ** (-1) - L_ort **(-1))\n",
    "    loss += L_ort * k_til(kernel, sigma, z, z, z_i, m)\n",
    "    return loss"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "96d619ae03109841",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# alpha_i needs some rescaling, named V[k, i] here\n",
    "def E_phi_grad(V, kernel, k_matrix_sum, sigma, z_i, z, L, L_ort, r, m):\n",
    "    loss = torch.zeros(z.shape)\n",
    "    # optimized gradient computation\n",
    "    par_z = torch.zeros([m, *z.shape])\n",
    "    k_til = torch.zeros([m])\n",
    "    for i in range(m):\n",
    "        par_z[i] += torch.autograd.grad(kernel(z_i[i], z, sigma), [z])[0]\n",
    "        k_til[i] += kernel(z_i[i], z, sigma)\n",
    "        for k in range(m):\n",
    "            par_z[i] -= (1/m) * torch.autograd.grad(kernel(z, z_i[k], sigma), [z])[0]\n",
    "            k_til[i] -= (1/m) * (kernel(z, z_i[k], sigma) + kernel(z_i[i], z_i[k], sigma))\n",
    "    \n",
    "    k_til += (1 / (m ** 2)) * k_matrix_sum\n",
    "    \n",
    "    alpha = cp.copy(V)\n",
    "    alpha[:, :r] *= (torch.sqrt(L[:r])[:, None]).t()\n",
    "    \n",
    "    for k in range(r):\n",
    "        for i in range(m):\n",
    "            loss += (alpha[i, k] * k_til[i])  * (alpha[i, k] * par_z[i]) * (L[k] ** (-1) - L_ort ** (-1))\n",
    "            \n",
    "    par_zz = torch.zeros([*z.shape])\n",
    "    for k in range(m):\n",
    "        par_zz -= (1/m) * torch.autograd.grad(kernel(z, z_i[k], sigma), [z])[0] # multiplication with 2 is missing    \n",
    "    loss += (L_ort ** (-1)) * par_zz\n",
    "    \n",
    "    return 2.0 * loss"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62ed9d64ecba2f5a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# alpha_i needs some rescaling, named V[k, i] here\n",
    "def E_phi_grad_opt(V, kernel, k_m, k_matrix_sum, sigma, z_i, z, L, L_ort, r, m):\n",
    "    loss = torch.zeros(z.shape)\n",
    "    \n",
    "    # lightspeed optimized gradient computation\n",
    "    par_z = torch.zeros([m, *z.shape])\n",
    "    kernel_ = torch.zeros([m])\n",
    "    for i in range(m):\n",
    "        par_z[i] = torch.autograd.grad(kernel(z_i[i], z, sigma), [z])[0]\n",
    "        kernel_[i] = kernel(z_i[i], z, sigma)\n",
    "\n",
    "    k_til = kernel_ - (1/m) * kernel_.sum(dim=0) - (1 /m) * k_m.sum(dim=1) + (1 / (m ** 2)) * k_matrix_sum\n",
    "    \n",
    "    par_z_sum = (1 / m) * par_z.sum(dim = 0)\n",
    "    kernel_til = lambda par_z, index : par_z[index] - par_z_sum\n",
    "    \n",
    "    alpha = cp.copy(V)\n",
    "    alpha[:, :r] *= (torch.sqrt(L[:r])[:, None]).t()\n",
    "    \n",
    "    for k in range(r):\n",
    "        for i in range(m):\n",
    "            loss += (alpha[i, k] * k_til[i])  * (alpha[i, k] * kernel_til(par_z, i)) * (L[k] ** (-1) - L_ort ** (-1))\n",
    "            \n",
    "    par_zz = torch.zeros([*z.shape])\n",
    "    for k in range(m):\n",
    "        par_zz -= (1/m) * par_z[k]\n",
    "    loss += (L_ort ** (-1)) * par_zz\n",
    "    \n",
    "    return 2.0 * loss"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "afcece65014161e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from src.util.timer import tic, toc\n",
    "import time\n",
    "\n",
    "def recon_preimg(V, kernel, sigma, x_i, x, r, m):\n",
    "    proj_phi_x = torch.zeros([m])\n",
    "    for i in range(m):\n",
    "        proj_phi_x[i] = kernel(x.double(), x_i[i], sigma)\n",
    "        \n",
    "    z = torch.rand(x.shape) # initial z for optimization, might need a more clever one...\n",
    "    z.requires_grad = True\n",
    "    \n",
    "    def loss(z):\n",
    "        sum = 0.0\n",
    "        for i in range(m):\n",
    "            sum += ((V ** 2)[i, :] * proj_phi_x * kernel(z, x_i[i], sigma)).sum()\n",
    "        \n",
    "        return -2.0 * sum\n",
    "    \n",
    "    max_it = 20\n",
    "    optimizer = torch.optim.LBFGS([z], max_eval=5, max_iter=10, lr=0.5)\n",
    "    \n",
    "    history = []\n",
    "    print(\"performing reconstruction optimization...\")\n",
    "    start = time.time()\n",
    "\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        L = loss(z.double())\n",
    "        l = L.detach().cpu().numpy()\n",
    "        print(\"loss\", l)\n",
    "        history.append(l)\n",
    "        L.backward()\n",
    "        return L\n",
    "\n",
    "    for i in range(max_it):\n",
    "        print(\"it \", i, \": \", end=\"\")\n",
    "        optimizer.step(closure)\n",
    "\n",
    "    print(\"Optimization (L-BFGS) time: \", round(time.time() - start, 2), \" seconds\")\n",
    "    return z"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d310c4939f80a7c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Actual running of the algorithms of shape priors and segmentation with continuous max-flow algorithm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81bc20208cd5c0ce"
  },
  {
   "cell_type": "code",
   "id": "cad7a8fd3f5bfbc4",
   "metadata": {},
   "source": [
    "from src.algs.arm import lv_indicator\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "# Some thinking is needed in the Mahalanobis distance part\n",
    "calc_type = torch.float\n",
    "def segment_left_ventricle(a_volume, a_opt_params, a_algo_params, a_plot=False, a_save_plot=False):\n",
    "    \"\"\"\n",
    "    Main function for segmenting the left ventricle from a reconstructed 3D volume (scalar-field)\n",
    "\n",
    "    Args:\n",
    "        a_volume (N, M, K): array_like\n",
    "                  To be segmented left ventricle SPECT volume (scalar-field)\n",
    "        \n",
    "        a_opt_params (dict): dict\n",
    "                      Parameter pack of the optimization. Upper limit of iterations num_iter,\n",
    "                      err_bound iteration error between steps limit, scaling for the gradient\n",
    "                      dampening is gamma, steps is the gradient step size\n",
    "        \n",
    "        a_algo_params (dict): dict\n",
    "                       Parameter pack of the Continuous Max-flow algorithm. TODO\n",
    "        \n",
    "        a_plot (bool): bool\n",
    "                Bit flag to use plotting of intermediate results or not\n",
    "\n",
    "        a_save_plot (bool): bool\n",
    "                     Bit flag to save plot of intermediate results or not\n",
    "    \"\"\"\n",
    "    num_iter, err_bound, gamma, steps = a_opt_params.values()\n",
    "    par_lambda, par_nu, c_zero, c_one, b_zero, b_one, z_i, sigma_inv, L, V, sigma_ort, sigma, first_cplx, min_shape_face_count, mean_shape, mean_shape_face, k_matrix_sum, k_matrix, kernel = a_algo_params.values()\n",
    "    m = len(z_i)\n",
    "    \n",
    "    norm_epsilon = 0.001\n",
    "\n",
    "    # mentioned in the paper\n",
    "    b_zero = c_zero\n",
    "    b_one = c_one\n",
    "\n",
    "    volume = a_volume\n",
    "    density_vol = volume / volume.sum()\n",
    "\n",
    "    rows, cols, height = a_volume.shape\n",
    "    im_size = rows * cols * height\n",
    "    \n",
    "    # initialization for CMF\n",
    "    if a_volume.dtype == torch.int32:\n",
    "        a_volume = a_volume.astype(calc_type)\n",
    "\n",
    "    alpha = 2 / (par_lambda + par_nu)\n",
    "\n",
    "    lv_params = dict(a=1, c=2, sigma=-1)\n",
    "    f_zero = torch.from_numpy(lv_indicator(a_volume, lv_params))\n",
    "    f_one = f_zero\n",
    "\n",
    "    im_eff = (par_lambda / (par_lambda + par_nu)) * a_volume + (par_nu / (par_lambda + par_nu)) \\\n",
    "             * (b_zero * (1 - f_one) + b_one * f_one)\n",
    "\n",
    "    Cs = (im_eff - c_zero) ** 2\n",
    "    Ct = (im_eff - c_one) ** 2\n",
    "    \n",
    "    u_prev = torch.zeros([rows, cols, height])\n",
    "    u = torch.where(Cs >= Ct, 1, 0).float()\n",
    "\n",
    "    ps = torch.minimum(Cs, Ct)\n",
    "    pt = ps\n",
    "\n",
    "    pp_y = torch.zeros((rows, cols + 1, height), dtype=calc_type)\n",
    "    pp_x = torch.zeros((rows + 1, cols, height), dtype=calc_type)\n",
    "    pp_z = torch.zeros((rows, cols, height + 1), dtype=calc_type)\n",
    "    div_p = torch.zeros((rows, cols, height), dtype=calc_type)\n",
    "\n",
    "    cmf_iter = 3\n",
    "    err_iter = torch.zeros(cmf_iter * num_iter, dtype=calc_type)\n",
    "    norm_u_iter = torch.zeros(num_iter + 1, dtype=calc_type)\n",
    "    \n",
    "    if a_plot is True:\n",
    "        plt.ion()\n",
    "\n",
    "        figure, axis = plt.subplots(2, 2)\n",
    "        figure.tight_layout()\n",
    "        \n",
    "        slice_num = torch.int32(a_volume.shape[0] / 2)\n",
    "\n",
    "        plot_obj_vol = axis[0, 0].imshow(a_volume[slice_num, :, :])\n",
    "        axis[0, 0].set_title(\"Left Ventricle Volume\")\n",
    "\n",
    "        plot_obj_seg = axis[0, 1].imshow(f_one[slice_num, :, :])\n",
    "        axis[0, 1].set_title(\"Segmentation\")\n",
    "\n",
    "        plot_obj_opt = axis[1, 1].imshow(u[slice_num, :, :])\n",
    "        axis[1, 1].set_title(\"Optimality\")\n",
    "\n",
    "        plot_obj_err = axis[1, 0].plot(err_iter[0])\n",
    "        axis[1, 0].set_title(\"Iteration error\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    for i in range(num_iter):\n",
    "        for j in range(cmf_iter):\n",
    "            pts = div_p - (ps - pt + u / gamma)\n",
    "\n",
    "            pp_y[:, 1:-1, :] += steps * (pts[:, 1:, :] - pts[:, :-1, :])\n",
    "            pp_x[1:-1, :, :] += steps * (pts[1:, :, :] - pts[:-1, :, :])\n",
    "            pp_z[:, :, 1:-1] += steps * (pts[:, :, 1:] - pts[:, :, :-1])\n",
    "\n",
    "            # the following steps give the projection to make |p(x)| <= alpha(x)\n",
    "            squares = pp_y[:, :-1, :] ** 2 + pp_y[:, 1:, :] ** 2\n",
    "            squares += pp_x[:-1, :, :] ** 2 + pp_x[1:, :, :] ** 2\n",
    "            squares += pp_z[:, :, :-1] ** 2 + pp_z[:, :, 1:] ** 2\n",
    "\n",
    "            gk = torch.sqrt(squares * .5)\n",
    "            gk = (gk <= alpha) + torch.logical_not(gk <= alpha) * (gk / alpha)\n",
    "            gk = 1 / gk\n",
    "\n",
    "            pp_y[:, 1:-1, :] = (.5 * (gk[:, 1:, :] + gk[:, :-1, :])) * (pp_y[:, 1:-1, :])\n",
    "            pp_x[1:-1, :, :] = (.5 * (gk[1:, :, :] + gk[:-1, :, :])) * (pp_x[1:-1, :, :])\n",
    "            pp_z[:, :, 1:-1] = (.5 * (gk[:, :, 1:] + gk[:, :, :-1])) * (pp_z[:, :, 1:-1])\n",
    "\n",
    "            div_p = pp_y[:, 1:, :] - pp_y[:, :-1, :]\n",
    "            div_p += pp_x[1:, :, :] - pp_x[:-1, :, :]\n",
    "            div_p += pp_z[:, :, 1:] - pp_z[:, :, :-1]\n",
    "\n",
    "            # update the source flow ps\n",
    "            pts = div_p + pt - u / gamma + 1 / gamma\n",
    "            ps = torch.minimum(pts, Cs)\n",
    "\n",
    "            # update the sink flow pt\n",
    "            pts = -div_p + ps + u / gamma\n",
    "            pt = torch.minimum(pts, Ct)\n",
    "\n",
    "            u_error = gamma * (div_p - ps + pt)\n",
    "            u -= u_error\n",
    "\n",
    "            u_error_normed = torch.sum(torch.abs(u_error)) / im_size\n",
    "            err_iter[cmf_iter * i + j] = u_error_normed\n",
    "\n",
    "            if a_plot is True:\n",
    "                plot_obj_opt.set_data(u[slice_num, :, :])\n",
    "                axis[1, 0].plot(err_iter[0: cmf_iter * i + j])\n",
    "                plt.draw()\n",
    "        \n",
    "        norm_u_iter[i + 1] = torch.linalg.norm(u)\n",
    "        \n",
    "        c_zero = torch.sum((1 - u) * im_eff) / torch.sum(1 - u)\n",
    "        c_one = torch.sum(u * im_eff) / (torch.sum(u))\n",
    "\n",
    "        im_mod = c_zero * (1 - u) + c_one * u\n",
    "\n",
    "        b_zero = torch.sum((1 - f_one) * im_mod) / (torch.linalg.norm(1 - f_one + norm_epsilon) ** 2)\n",
    "        b_one = torch.sum(f_one * im_mod) / (torch.linalg.norm(f_one + norm_epsilon) ** 2)\n",
    "                \n",
    "        print(\"u sum: \", u.sum(), \"u max: \", u.max(), \"u min: \", u.min(), \"u count:\", (u > 0).sum())\n",
    "        \n",
    "        zero_volume_boundary(u, a_width=2)\n",
    "        #vert_vol, tri_vol = mcubes.marching_cubes(u.numpy(), 0.1)\n",
    "        vert_vol, tri_vol, _, _ =  measure.marching_cubes(u.numpy(), 0.2)  # 0.5 for cardioc, 0.2 for mph, 0.1 for parallel, 0.1 for cardiod\n",
    "        cv, nv, cf, nf = pcu.connected_components(vert_vol, tri_vol.astype(np.int32))\n",
    "        \n",
    "        num_components = nv.size\n",
    "        print(\"Connected components: \", num_components)\n",
    "        print(\"Iteration: \", i, \"Norm diff: \", torch.abs(norm_u_iter[i + 1] - norm_u_iter[i]))\n",
    "        \n",
    "        f_one = torch.zeros((rows, cols, height))\n",
    "        \n",
    "        component = 0\n",
    "\n",
    "        while (component < num_components)  and (torch.abs(norm_u_iter[i + 1] - norm_u_iter[i]) < 0.178) or (i + 1) == num_iter: #   3 for parallel images\n",
    "            nu = 1e-2\n",
    "            \n",
    "            if component >= num_components:\n",
    "                break\n",
    "\n",
    "            if num_components > 1:\n",
    "                component_face_count = nf[component]\n",
    "            else:\n",
    "                component_face_count = nf\n",
    "            \n",
    "            v_decimate, f_decimate, v_correspondence, f_correspondence =\\\n",
    "                pcu.decimate_triangle_mesh(vert_vol, tri_vol[cf == component].astype(np.int32), min(min_shape_face_count.numpy(), component_face_count))\n",
    "                        \n",
    "            z = torch.from_numpy(v_decimate / cols)\n",
    "            z.requires_grad = True\n",
    "            \n",
    "            # renorm to size 1 and translate it to center_point\n",
    "            z_dist = cdist(z.detach().numpy(), z.detach().numpy(), 'euclidean')\n",
    "            max_real_size = z_dist.max() * cols\n",
    "            if max_real_size <= 20:  # dummy \"size\" selection 15 for parallel geometries, 20 for mph -> sharpen this\n",
    "                print(\"Skipping object with diameter: \", max_real_size)\n",
    "                component = component  + 1\n",
    "                continue\n",
    "            \n",
    "            z_scaled = z * (1.0 / (z_dist.max()))\n",
    "            z_translation = z_scaled.mean(dim=0) - torch.from_numpy(centering_point)\n",
    "            \n",
    "            # Project current shape on the mean shape as in [1]\n",
    "            pc_fix = PointCloud(mean_shape.detach().numpy(), columns=[\"x\", \"y\", \"z\"])\n",
    "            pc_mov = PointCloud((z_scaled - z_translation).detach().numpy(), columns=[\"x\", \"y\", \"z\"])\n",
    "            icp = SimpleICP()\n",
    "            icp.add_point_clouds(pc_fix, pc_mov)\n",
    "            H, proj_mean_icp, rigid_body_transformation_params, distance_residuals = icp.run(max_overlap_distance=1)\n",
    "            # add reorientation based registration here\n",
    "            \n",
    "            proj_mean = torch.from_numpy(proj_mean_icp)\n",
    "            proj_mean.requires_grad = True\n",
    "            \n",
    "            grad_E = E_phi_grad_opt(V, kernel, k_matrix, k_matrix_sum, sigma, z_i, proj_mean, L, sigma_ort, first_cplx, m)\n",
    "            \n",
    "            # the last terms in the gradient calculation\n",
    "            # d til_z / d_z_c * d_z_c / d_z\n",
    "            Rot = H[:-1, :-1]\n",
    "            translation = H[:-1, -1]\n",
    "            \n",
    "            it_shape = ((proj_mean - nu * grad_E - torch.from_numpy(translation)) @ torch.from_numpy(Rot) + z_translation) * z_dist.max()\n",
    "            print(\"Translation:\", translation, \"Normalization factor: \", z_dist.max(), \"Mean translation: \", z_translation)\n",
    "                        \n",
    "            import matplotlib.pyplot as plt\n",
    "            %matplotlib notebook        \n",
    "            fig = plt.figure()\n",
    "            ax1 = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "            plot_1 = z.detach().numpy()\n",
    "            ax1.scatter(plot_1[:, 0], plot_1[:, 1], plot_1[:, 2])\n",
    "            ax2 = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "            plot_2 = it_shape.detach().numpy()\n",
    "            ax2.shareview(ax1)\n",
    "            ax2.scatter(plot_2[:, 0], plot_2[:, 1], plot_2[:, 2])\n",
    "            # \n",
    "            # component = component + 1\n",
    "            # continue\n",
    "            \n",
    "            # voxelization and bounds checking\n",
    "            ijk = pcu.voxelize_triangle_mesh((it_shape).detach().numpy() * cols, f_decimate.astype(np.int32), 1, [0., 0., 0.])\n",
    "            ijk = ijk[np.sum(np.logical_and(ijk >=0, ijk < cols), axis=1) == 3, :]\n",
    "            ijk = ijk[ijk[:, 0] < rows] # more likely that the axial dim is different\n",
    "            \n",
    "            f_one[ijk[:, 0], ijk[:, 1], ijk[:, 2]] = 1\n",
    "\n",
    "            print(\"component: \", component, \" energy min: \", grad_E.min(), \" energy max: \", grad_E.max(),\n",
    "                  \" shape prior count: \", f_one.sum(), \" shape prior mean pos:\", (it_shape).mean())\n",
    "            \n",
    "            component = component + 1\n",
    "    \n",
    "        if a_plot is True:\n",
    "            plot_obj_seg.set_data(f_one[slice_num, :, :])\n",
    "            plt.draw()\n",
    "\n",
    "            if a_save_plot is True:\n",
    "                name = \"..\\\\..\\\\left_ventricle_\" + str(i) + \".png\"\n",
    "                plt.savefig(name, bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "        im_eff = (par_lambda / (par_lambda + par_nu)) * a_volume + (par_nu / (par_lambda + par_nu)) \\\n",
    "                 * (b_zero * (1 - f_one) + b_one * f_one)\n",
    "        \n",
    "        H = torch.where(u > 0, 1, 0)\n",
    "        Cs = (im_eff - c_zero) ** 2 * torch.kl_div(H * density_vol, (1 - H) * density_vol).sum()\n",
    "        Ct = (im_eff - c_one) ** 2 * torch.kl_div((1 - H) * density_vol, H * density_vol).sum()\n",
    "    \n",
    "    return u, err_iter, num_iter, f_one"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ee0ef705dd567ec7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# might need to try with different models from CV, e.g.: MS, Potts\n",
    "from geomloss import SamplesLoss\n",
    "eps = 5 * 1e-3\n",
    "loss_unbalanced = SamplesLoss(loss='sinkhorn', p=2, blur=eps, scaling=0.95)\n",
    "sigma = 5 * 1e0\n",
    "# k = lambda x, y, sigma : torch.exp(-loss(x, y) ** 2 / (2 * sigma ** 2))\n",
    "k = lambda x, y, sigma : torch.exp(-sigma * loss_unbalanced(x, y))\n",
    "centering_point = np.array([0.45, 0.45, 0.45])\n",
    "\n",
    "z_i, sigma_inv, L, V, sigma_ort, sigma, first_cplx, min_shape_face_count, mean_shape, mean_shape_face, k_matrix_sum, k_matrix  = nonlinear_shape_prior(shape_priors, kernel=k, sigma=sigma, centering_point=centering_point)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(V.max())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c64f2506d5c51aab",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import pyprof\n",
    "import torch.cuda.profiler as profiler \n",
    "\n",
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA,\n",
    "    ],\n",
    "    with_flops=True) as prof:\n",
    "        opt_params = dict(num_iter=10, err_bound=0, gamma=1e-2, steps=1e-1)\n",
    "        cmf_params = dict(par_lambda=2, par_nu=3, c_zero=0.3, c_one=0.7, b_zero=1e-1, b_one=1e1,\n",
    "                          z_i=z_i, sigma_inv=sigma_inv, L=L, V=V, sigma_ort=sigma_ort, sigma=sigma, first_cplx=first_cplx, min_shape_face_count=min_shape_face_count, mean_shape=mean_shape, mean_shape_face=mean_shape_face,k_matrix_sum=k_matrix_sum, k_matrix=k_matrix, kernel=k)\n",
    "        lam, err_iter, num_iter, lam_shape_prior = segment_left_ventricle(a_volume=torch.from_numpy(lv_volume), a_opt_params=opt_params, a_algo_params=cmf_params)\n",
    "\n",
    "events = prof.events()\n",
    "cmf_shape_flops = sum([int(evt.flops) for evt in events]) \n",
    "print(\"Runtime FLOPs: \", cmf_shape_flops)"
   ],
   "metadata": {},
   "id": "5605c061303e7cd7",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(prof)\n",
    "events = prof.events()\n",
    "cmf_shape_flops = sum([int(evt.flops) for evt in events]) \n",
    "print(\"Runtime FLOPs: \", cmf_shape_flops)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "318e18bee54d4d0b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib notebook\n",
    "slice = 30\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(1, 3, 1)\n",
    "ax1.imshow(lam[slice, :, :])\n",
    "ax2 = fig.add_subplot(1, 3, 2)\n",
    "ax2.imshow(lv_volume[slice, :, :])\n",
    "ax3 = fig.add_subplot(1, 3, 3)\n",
    "ax3.imshow(lam_shape_prior[slice, :, :])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c6cf2b956bfc7dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# inspection figures in the paper\n",
    "fill_value = 3  # Fill starting from [0, 0, 0] with the value 2\n",
    "label_prior = cp.copy(lam_shape_prior)\n",
    "filled_myocard = pcu.flood_fill_3d(label_prior, [0, 0, 0], fill_value)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib notebook\n",
    "slice = 30\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax1.imshow(filled_myocard[slice, :, :])\n",
    "ax1 = fig.add_subplot(1, 2, 2)\n",
    "ax1.imshow(lv_volume[slice, :, :])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "446d6c6f66d6fe65",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# fig = plt.figure()\n",
    "# fig.set_size_inches((1, 1))\n",
    "# ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "# ax.set_axis_off()\n",
    "# fig.add_axes(ax)\n",
    "# # turkey pat 10, (TRA 30, 17:37, 30:50), (VLA 23:43, 28, 30:50), (SA 23:43, 17:37, 42)\n",
    "# \n",
    "# plt.imshow(lv_volume[23:43, 17:37, 42], aspect='equal')\n",
    "# plt.savefig(\"/home/jackson/GIT/ELTE/papers/left_ventricle_segmentation/allerton_2023/images/\" + \"patient_10_sa\" + \".png\", bbox_inches='tight', pad_inches=0)\n",
    "# plt.close()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "43f415ff78160915",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reconstructing one of the eigenvalues/eigenvectors to check the correctness of embedding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8eba30ecf30095d5"
  },
  {
   "cell_type": "code",
   "source": [
    "from geomloss import SamplesLoss\n",
    "eps = 5 * 1e-1\n",
    "loss = SamplesLoss(loss='sinkhorn', p=2, blur=eps)\n",
    "sigma = 5 * 1e-5\n",
    "k = lambda x, y, sigma : torch.exp(-loss(x, y) ** 2 / (2 * sigma ** 2))\n",
    "\n",
    "z_i, sigma_inv, L, V, sigma_ort, _, first_cplx, min_shape_face_count, mean_shape, mean_shape_face, k_matrix_sum, k_matrix = nonlinear_shape_prior(shape_priors, kernel=k, sigma=sigma, centering_point=centering_point)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "58d94dfa1375c915",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# input_shape = torch.rand([400, 3])\n",
    "verts, faces = mcubes.marching_cubes(lv_volume, 0.5)\n",
    "v_decimate, f_decimate, v_correspondence, f_correspondence = pcu.decimate_triangle_mesh(verts, faces.astype(np.int32), min_shape_face_count)\n",
    "input_shape = torch.from_numpy(v_decimate / height)\n",
    "print(loss(input_shape.double(), z_i[0]))\n",
    "print(loss(z_i[1], z_i[-1]))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c76575dd716cab74",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "sigma = 5 * 1e-1\n",
    "pre_z = recon_preimg(V, k, sigma, z_i, input_shape, first_cplx, len(z_i)).detach().numpy()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16ea8055c7635faf",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "model_ind = 0\n",
    "model = z_i[model_ind].detach().numpy()\n",
    "\n",
    "input = input_shape.detach().numpy()\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "ax1 = fig.add_subplot(1, 3, 1, projection='3d')\n",
    "ax1.scatter(pre_z[:, 0], pre_z[:, 1], pre_z[:, 2])\n",
    "\n",
    "ax2 = fig.add_subplot(1, 3, 2, projection='3d')\n",
    "ax2.shareview(ax1)\n",
    "ax2.scatter(input[:, 0], input[:, 1], input[:, 2])\n",
    "\n",
    "ax3 = fig.add_subplot(1, 3, 3, projection='3d')\n",
    "ax3.shareview(ax1)\n",
    "ax3.scatter(model[:, 0], model[:, 1], model[:, 2])\n",
    "\n",
    "print(\"Reconstructed surface shape: \", pre_z.shape, \" Input shape: \", input_shape.shape)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6fa47da72fe9632d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "recon_model_plot = pre_z * width\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "ax1 = fig.add_subplot(1, 1, 1, projection='3d')\n",
    "ax1.scatter(recon_model_plot[:, 0], recon_model_plot[:, 1], recon_model_plot[:, 2])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "72612183518a75e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check optimal transport projection"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42941806de2759b1"
  },
  {
   "cell_type": "code",
   "source": [
    "lv_params = dict(a=1, c=2, sigma=-1)\n",
    "vol = lv_indicator(np.zeros([64, 64, 64]), lv_params)\n",
    "input_shape_verts, input_shape_faces = mcubes.marching_cubes(vol, 0.0)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29b9531700b64063",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "x_shape = torch.from_numpy(input) / 64 # torch.from_numpy(input_shape_verts / 64)\n",
    "y_shape = mean_shape\n",
    "\n",
    "print(\"Source shape mean:\", x_shape.mean(), \"Target shape mean:\", y_shape.mean())\n",
    "translation = y_shape.mean(dim=0) - x_shape.mean(dim=0)\n",
    "print(\"Translation:\", translation)\n",
    "print(\"Source shape mean with translation:\", (x_shape + translation).mean())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f84558f2fb5b8d0d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(torch.linalg.norm(x_shape))\n",
    "print(torch.linalg.norm(y_shape))   "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f737cd07d9f99aaa",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "x = (x_shape).double().t().flatten()[:, None] / torch.linalg.norm(x_shape)\n",
    "y = y_shape.double().flatten()[:, None] / torch.linalg.norm(y_shape)\n",
    "\n",
    "a = torch.ones(x.shape)\n",
    "b = torch.ones(y.shape)\n",
    "\n",
    "print(\"Source mean: \", x.mean(), \"Target mean: \", y.mean())\n",
    "print(x.shape, y.shape)\n",
    "\n",
    "N, M, D = x.shape[0], y.shape[0], x.shape[1]\n",
    "p = 2\n",
    "blur = 5 * 1e-2\n",
    "\n",
    "OT_solver = SamplesLoss(loss = \"sinkhorn\", p = p, blur = blur, scaling=0.2, debias=False, potentials=True)\n",
    "F, G = OT_solver(x, y)  # Dual potentials\n",
    "\n",
    "x_i, y_j = x.view(N, 1, D), y.view(1, M, D)\n",
    "a_i, b_j = a.view(N, 1), b.view(1, M)\n",
    "F_i, G_j = F.view(N, 1), G.view(1, M)\n",
    "\n",
    "C_ij = (1 / p) * ((x_i - y_j) ** p).sum(-1)\n",
    "eps = blur ** p\n",
    "P_ij = ((F_i + G_j - C_ij) / eps).exp()\n",
    "\n",
    "proj_mean = x.flatten() @ (P_ij)\n",
    "proj_mean = torch.reshape(proj_mean, mean_shape.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e5462903ed203fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "torch.norm(mean_shape - proj_mean)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "89af1aac509a3903",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# try estimating the rotation mx with wasserstein metric\n",
    "x = x_shape.double()\n",
    "y = y_shape.double()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f79cc0b094be4f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "x_dist = cdist(x.numpy(), x.numpy(), 'euclidean')\n",
    "y_dist = cdist(y.numpy(), y.numpy(), 'euclidean')\n",
    "\n",
    "print(x.max(), y.max())\n",
    "print(x.min(), y.min())\n",
    "print(x.mean(), y.mean())\n",
    "print(x_dist.max(), y_dist.max())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "75fabee1a036ceb",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "x_scale = 1.0 / x_dist.max()\n",
    "y_scale = 1.0 / y_dist.max()\n",
    "\n",
    "print(x_scale)\n",
    "print(y_scale)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb3d9515cda8d4f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "x_scaled = x * x_scale\n",
    "y_scaled = y * y_scale\n",
    "\n",
    "x_scaled_mean = x_scaled.mean(dim=0)\n",
    "y_scaled_mean = y_scaled.mean(dim=0)\n",
    "\n",
    "x_translation = torch.tensor([0.45, 0.45, 0.45]) - x_scaled_mean\n",
    "y_translation = torch.tensor([0.45, 0.45, 0.45]) - y_scaled_mean"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ed67e34565b09c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "ax1 = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "plot_input_shape_verts = x_scaled + x_translation\n",
    "ax1.scatter(plot_input_shape_verts[:, 0], plot_input_shape_verts[:, 1], plot_input_shape_verts[:, 2])\n",
    "\n",
    "ax2 = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "ax2.shareview(ax1)\n",
    "output_shape_verts = (y_scaled + y_translation).detach().numpy()\n",
    "ax2.scatter(output_shape_verts[:, 0], output_shape_verts[:, 1], output_shape_verts[:, 2])\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1377e3cdde5078a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# try out ICP\n",
    "from simpleicp import PointCloud, SimpleICP\n",
    "\n",
    "pc_fix = PointCloud((y_scaled + y_translation).detach().numpy(), columns=[\"x\", \"y\", \"z\"])\n",
    "pc_mov = PointCloud(x_scaled + x_translation, columns=[\"x\", \"y\", \"z\"])\n",
    "\n",
    "# Create simpleICP object, add point clouds, and run algorithm!\n",
    "icp = SimpleICP()\n",
    "icp.add_point_clouds(pc_fix, pc_mov)\n",
    "H, proj_mean_icp, rigid_body_transformation_params, distance_residuals = icp.run(max_overlap_distance=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "581306b2b4d2edb4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "ax1 = fig.add_subplot(1, 3, 1, projection='3d')\n",
    "orig_input_shape = x.detach().numpy()\n",
    "ax1.scatter(orig_input_shape[:, 0], orig_input_shape[:, 1], orig_input_shape[:, 2])\n",
    "\n",
    "ax2 = fig.add_subplot(1, 3, 2, projection='3d')\n",
    "ax2.shareview(ax1)\n",
    "plot_input_shape_verts = proj_mean_icp\n",
    "ax2.scatter(plot_input_shape_verts[:, 0], plot_input_shape_verts[:, 1], plot_input_shape_verts[:, 2])\n",
    "\n",
    "ax3 = fig.add_subplot(1, 3, 3, projection='3d')\n",
    "ax3.shareview(ax1)\n",
    "output_shape_verts = (y_scaled + y_translation).detach().numpy()\n",
    "ax3.scatter(output_shape_verts[:, 0], output_shape_verts[:, 1], output_shape_verts[:, 2])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f60165fd2fc22fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(H)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "df7e817d2e16eb49",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# test inverse rotation\n",
    "Rot = H[:-1, :-1]\n",
    "print(Rot)\n",
    "inv_Rot = np.linalg.inv(Rot)\n",
    "print(inv_Rot)\n",
    "translation = H[:-1, -1]\n",
    "print(translation)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bad675f8be4f2eef",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "inv_rot_input_shape = x.detach().numpy()\n",
    "ax1.scatter(inv_rot_input_shape[:, 0], inv_rot_input_shape[:, 1], inv_rot_input_shape[:, 2])\n",
    "\n",
    "ax3 = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "ax3.shareview(ax1)\n",
    "output_shape_verts = x.detach().numpy()\n",
    "ax3.scatter(output_shape_verts[:, 0], output_shape_verts[:, 1], output_shape_verts[:, 2])\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63c6f4b292fd76b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "ax1 = fig.add_subplot(1, 3, 1, projection='3d')\n",
    "plot_input_shape_verts = (x_shape + translation) * 64\n",
    "ax1.scatter(plot_input_shape_verts[:, 0], plot_input_shape_verts[:, 1], plot_input_shape_verts[:, 2])\n",
    "\n",
    "ax2 = fig.add_subplot(1, 3, 2, projection='3d')\n",
    "ax2.shareview(ax1)\n",
    "output_shape_verts = proj_mean.detach().numpy()\n",
    "ax2.scatter(output_shape_verts[:, 0], output_shape_verts[:, 1], output_shape_verts[:, 2])\n",
    "\n",
    "ax3 = fig.add_subplot(1, 3, 3, projection='3d')\n",
    "ax3.shareview(ax1)\n",
    "mean_shape_verts = mean_shape * 64\n",
    "ax3.scatter(mean_shape_verts[:, 0], mean_shape_verts[:, 1], mean_shape_verts[:, 2])\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da4c505db8caf7a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Based on chosen OT check theoretical facts"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84c2c57efc603f50"
  },
  {
   "cell_type": "code",
   "source": [
    "def kernel_matrix(z_i, kernel, sigma):\n",
    "    m = len(z_i)\n",
    "    \n",
    "    K = torch.zeros([m, m])\n",
    "    E = (1 / m) * torch.ones([m, m])\n",
    "    \n",
    "    for i in range(m):\n",
    "        for j in range(m):\n",
    "            K[i, j] = kernel(z_i[i], z_i[j], sigma)\n",
    "    K_til= (torch.eye(m) - E) @ K @ (torch.eye(m) - E)\n",
    "    \n",
    "    L, V = torch.linalg.eigh(K_til)\n",
    "    L = torch.flip(L, [0])\n",
    "    V = torch.fliplr(V)\n",
    "\n",
    "    limit_val = 1e-6\n",
    "    if (L <= limit_val).any():\n",
    "        first_cplx = torch.where(L <= limit_val)[0][0]\n",
    "        sigma_ort = L[first_cplx - 1] / 2.0\n",
    "        \n",
    "        L[first_cplx:] = 0.0\n",
    "        V[:, first_cplx:] = 0.0\n",
    "        reg_mx = torch.eye(K.shape[0])\n",
    "    \n",
    "    Sigma_ort = V @ torch.diag(L) @ V.t() + sigma_ort * (reg_mx - V @ V.t())\n",
    "        \n",
    "    return K_til, K, Sigma_ort, V, L, K.sum(), first_cplx, sigma_ort"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a9ce0baa76efa13",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from geomloss import SamplesLoss\n",
    "\n",
    "eps = 5 * 1e-3\n",
    "\n",
    "# balanced and unbalanced loss fns\n",
    "loss_balanced = SamplesLoss(loss='sinkhorn', p=2, blur=eps)\n",
    "loss_unbalanced = SamplesLoss(loss='sinkhorn', p=2, blur=eps, reach=0.7, scaling=0.95)\n",
    "\n",
    "sigma = 5 * 1e0\n",
    "k_balanced = lambda x, y, sigma : torch.exp(-sigma * loss_balanced(x, y))\n",
    "k_unbalanced = lambda x, y, sigma : torch.exp(-sigma * loss_unbalanced(x, y))\n",
    "\n",
    "K_til_balanced, K_balanced, Sigma_balanced, V_balanced, L_balanced, K_sum_balanced, first_cplx_balanced, sigma_ort_balanced = kernel_matrix(z_i, k_balanced, sigma)\n",
    "K_til_unbalanced, K_unbalanced, Sigma_unbalanced, V_unbalanced, L_unbalanced, K_sum_unbalanced, first_cplx_unbalanced, sigma_ort_unbalanced = kernel_matrix(z_i, k_unbalanced, sigma)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f999885c98d2dc6",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "L_bal = torch.linalg.cholesky(K_til_balanced)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea1dfe353e6266ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "L_ubal = torch.linalg.cholesky(K_til_unbalanced)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54ef6ff5b8445a8a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(2, 3, 1)\n",
    "ax1.imshow(K_til_balanced)\n",
    "ax2 = fig.add_subplot(2, 3, 2)\n",
    "ax2.imshow(K_balanced)\n",
    "ax3 = fig.add_subplot(2, 3, 3)\n",
    "ax3.imshow(Sigma_balanced)\n",
    "\n",
    "ax4 = fig.add_subplot(2, 3, 4)\n",
    "ax4.imshow(K_til_unbalanced)\n",
    "ax5 = fig.add_subplot(2, 3, 5)\n",
    "ax5.imshow(K_unbalanced)\n",
    "ax6 = fig.add_subplot(2, 3, 6)\n",
    "ax6.imshow(Sigma_unbalanced)\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b621e3c868f473e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "L_bal_, V_bal_ = torch.linalg.eigh(K_til_balanced)\n",
    "L_bal = torch.flip(L_bal_, [0])\n",
    "V_bal = torch.flip(V_bal_, [0, 1])\n",
    "print(L)\n",
    "\n",
    "L_ubal_, V_ubal_ = torch.linalg.eigh(K_til_unbalanced)\n",
    "L_ubal = torch.flip(L_ubal_, [0])\n",
    "V_ubal = torch.flip(V_ubal_, [0, 1])\n",
    "print(L)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48c8eaa31ff00b9e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "for i in range(len(z_i)):\n",
    "    print(V_bal[:, i] @ V_bal[:, i])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dee2f196cbcd988a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(torch.linalg.det(K_til_balanced))\n",
    "print(torch.linalg.det(K_balanced))\n",
    "print(torch.linalg.det(Sigma_balanced))\n",
    "print(torch.linalg.det(K_til_unbalanced))\n",
    "print(torch.linalg.det(K_unbalanced))\n",
    "print(torch.linalg.det(Sigma_unbalanced))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b004147357e54a43",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(1, 3, 1)\n",
    "ax1.imshow(V_bal_)\n",
    "ax2 = fig.add_subplot(1, 3, 2)\n",
    "ax2.imshow(V_bal)\n",
    "ax3 = fig.add_subplot(1, 3, 3)\n",
    "ax3.imshow(torch.fliplr(V_bal_))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54ff26b8aa0bffa8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Check gradient, mean projection and test point projection as well"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6a60b54a39fa9e9"
  },
  {
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "z = torch.from_numpy(input) # torch.from_numpy(mean_shape / 64).double()\n",
    "z.requires_grad = True\n",
    "\n",
    "start = time.time()\n",
    "grad_E = E_phi_grad(V_unbalanced, k_unbalanced, K_sum_unbalanced, 5 * 1e0, z_i, z, L_unbalanced, sigma_ort_unbalanced, first_cplx_unbalanced, len(z_i))\n",
    "print(\"Naive gradient calculation time: \", round(time.time() - start, 2), \" seconds\")\n",
    "\n",
    "start = time.time()\n",
    "grad_E_opt = E_phi_grad_opt(V_unbalanced, k_unbalanced, K_unbalanced, K_sum_unbalanced, 5 * 1e0, z_i, z, L_unbalanced, sigma_ort_unbalanced, first_cplx_unbalanced, len(z_i))\n",
    "print(\"Optimized gradient calculation time: \", round(time.time() - start, 2), \" seconds\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c42a385fa5a377bd",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Lazy gradient max: \", grad_E.max(), \"Lazy gradient min: \", grad_E.min())\n",
    "print(\"Opt gradient max: \", grad_E_opt.max(), \"Opt gradient min: \", grad_E_opt.min())\n",
    "print(\"Lazy grad norm: \", torch.linalg.norm(grad_E))\n",
    "print(\"Opt grad norm: \", torch.linalg.norm(grad_E_opt))\n",
    "print(\"Norm difference: \", torch.linalg.norm(grad_E - grad_E_opt))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a135efe04c48be8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "nu = 1e0\n",
    "\n",
    "it_shape = z * 64 - nu * grad_E # z * 64 - grad_E #torch.from_numpy(input).double() # proj_mean * 64 - grad_E\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(1, 1, 1, projection='3d')\n",
    "plot = (it_shape).detach().numpy()\n",
    "ax1.scatter(plot[:, 0], plot[:, 1], plot[:, 2])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "472428b1834741f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "source_shape = z.detach().numpy()\n",
    "print(source_shape.max(), source_shape.min(), source_shape.mean())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5450d9f0e2994f02",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "result_shape = it_shape.detach().numpy()\n",
    "print(result_shape.max(), result_shape.min(), result_shape.mean())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83749d442c5340b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "(input.mean())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e29fadef22e59771",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "(input_shape_verts / 64).mean()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "adfd0d7e069ea62b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "(mean_shape).mean()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b48803a68cdf5cce",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "(z).mean()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cda6735188361e4d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Now scalarize the model and check variability of statistical model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9af4f92f484c5cd1"
  },
  {
   "cell_type": "code",
   "source": [
    "import pymeshlab\n",
    "pc = pymeshlab.MeshSet()\n",
    "pc.add_mesh(pymeshlab.Mesh(recon_model_plot))\n",
    "pc.compute_normal_for_point_clouds()\n",
    "pc.generate_surface_reconstruction_ball_pivoting()\n",
    "mesh = pc.current_mesh()\n",
    "\n",
    "faces = mesh.face_matrix()\n",
    "f_one = torch.zeros(lv_volume.shape)\n",
    "ijk = pcu.voxelize_triangle_mesh(recon_model_plot, faces.astype(np.int32), 1, [0., 0., 0.])\n",
    "\n",
    "f_one[ijk[:, 0], ijk[:, 1], ijk[:, 2]] = 1\n",
    "print(f_one.sum())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e9636b3e0828c08",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_trisurf(recon_model_plot[:, 0], recon_model_plot[:, 1], recon_model_plot[:, 2], triangles = faces.astype(np.int32), edgecolor=[[0,0,0]], linewidth=1.0, alpha=0.0, shade=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "484a2fc1b4499257",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "pts = np.random.rand(9000, 3) * (recon_model_plot.max(0) - recon_model_plot.min(0)) + recon_model_plot.min(0)\n",
    "\n",
    "sdfs, face_ids, barycentric_coords = pcu.signed_distance_to_mesh(pts.astype(np.float32), recon_model_plot, faces.astype(np.int32))\n",
    "sdf_vol = np.zeros([64, 64, 64])\n",
    "sdf_vol[pts[:, 0].astype(np.int32), pts[:, 1].astype(np.int32), pts[:, 2].astype(np.int32)] = sdfs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "950a58fe98953576",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(sdfs.max())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d6b6a7c281fe8c03",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "slice = 55\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.imshow(f_one[slice, :, :])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a5068c27af6eed01",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reconstruct model parameters from measurement"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd80e30543a7b7f2"
  },
  {
   "cell_type": "code",
   "source": [
    "# takes too long, running an EM algorithm here\n",
    "recon_coords = torch.from_numpy(pre_z)\n",
    "volume = np.zeros([*lv_volume.shape])\n",
    "\n",
    "sigma = 5 * 1e0\n",
    "\n",
    "# parameter limits\n",
    "# wall_thickness = np.random.uniform(0.3, 1.0, num_prior)\n",
    "# rot_angles = np.random.uniform(0, 2 * np.pi, num_prior)\n",
    "# curvature = np.random.uniform(1.5, 3, num_prior)\n",
    "# sigmas = np.random.uniform(-0.5, -1, num_prior)\n",
    "\n",
    "points = torch.zeros([len(shape_priors), 3])\n",
    "N = len(shape_priors)\n",
    "data_size = int(N * N)\n",
    "euclidean_dist = torch.zeros(data_size)\n",
    "feature_dist = torch.zeros(data_size)\n",
    "\n",
    "for i in range(len(shape_priors)):\n",
    "    points[i] = torch.tensor([wall_thickness[i], curvature[i], sigmas[i]])\n",
    "    \n",
    "for i in range(len(shape_priors)):\n",
    "    for j in range(len(shape_priors)):\n",
    "        euclidean_dist[i * (len(shape_priors)) + j] = torch.cdist(points[i, None], points[j, None], p=2)\n",
    "        feature_dist[i * (len(shape_priors)) + j] = k(z_i[i], z_i[j], sigma)\n",
    "            \n",
    "from scipy import interpolate\n",
    "\n",
    "f = interpolate.interp1d(feature_dist, euclidean_dist, fill_value='extrapolate')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6f1f3e7438860c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Now estimate coordinates based on feature space distance"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b5318dc41e4d6649"
  },
  {
   "cell_type": "code",
   "source": [
    "feature_dist_recon = torch.zeros(N)\n",
    "euclidean_dist_recon = torch.zeros(N)\n",
    "for i in range(N):\n",
    "    feature_dist_recon[i] = k(recon_coords.double(), z_i[i].double(), sigma)\n",
    "    euclidean_dist_recon[i] = torch.from_numpy(f(feature_dist_recon[i]))\n",
    "\n",
    "euclidean_dist_all = torch.cat([euclidean_dist, euclidean_dist_recon])\n",
    "feature_dist_all = torch.cat([feature_dist, feature_dist_recon])\n",
    "sorted, indices = torch.sort(feature_dist_all, 0)\n",
    "\n",
    "print(euclidean_dist_all[indices].shape)\n",
    "print(feature_dist.shape)\n",
    "# print(euclidean_dist_all)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "fig = plt.figure()\n",
    "plt.plot(feature_dist, euclidean_dist, 'o', sorted, euclidean_dist_all[indices], '-')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28f1354c12c3cc7e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(feature_dist_recon)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8df197ef72e3d90a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import math\n",
    "import scipy.optimize\n",
    "\n",
    "def func(par):\n",
    "    x1, x2, x3 = par\n",
    "    eqs = torch.zeros(N)\n",
    "    for i in range(N):\n",
    "        a = points[i, 0]\n",
    "        b = points[i, 1]\n",
    "        c = points[i, 2]\n",
    "        \n",
    "        eqs[i] = (x1 - a) ** 2 + (x2 - b) ** 2 + (x3 - c) ** 2\n",
    "    \n",
    "    return eqs\n",
    "\n",
    "def system(x, b):\n",
    "    return (func(x) - b ** 2)\n",
    "\n",
    "x = scipy.optimize.leastsq(system, np.asarray((0.6, 2.0, -0.7)), args=(euclidean_dist_recon), full_output=True)[0]\n",
    "\n",
    "print(x)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c52ed810677fac9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing the gradient calculation based on scalarized model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a43aa357756b86e0"
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_trisurf(verts[:, 0], verts[:, 1], verts[:, 2], triangles = faces.astype(np.int32), edgecolor=[[0,0,0]], linewidth=1.0, alpha=0.0, shade=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ddc8204e9251c4e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "input_shape.requires_grad = True\n",
    "grad_E = E_phi_grad(V, k, k_matrix_sum, sigma, z_i, input_shape.double(), L, sigma_ort, first_cplx, len(z_i)).detach().numpy()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a1f40b20678b329",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter(grad_E[:, 0], grad_E[:, 1], grad_E[:, 2])\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9a67eb6b12178db",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "plt.close(\"all\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed6767ee218331b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib notebook        \n",
    "# fig = plt.figure()\n",
    "# ax1 = fig.add_subplot(1, 1, 1, projection='3d')\n",
    "# plot = (it_shape).detach().numpy()\n",
    "# ax1.scatter(plot[:, 0], plot[:, 1], plot[:, 2])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec66f597426c81d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Computing metric for the black-box mixed dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7b0781b259735f4"
  },
  {
   "cell_type": "code",
   "source": [
    "def compute_metrics(prediction, target):\n",
    "    epsilon=1e-9\n",
    "    \n",
    "    num_runs = len(prediction)\n",
    "    \n",
    "    precisions = np.zeros(num_runs)\n",
    "    recalls = np.zeros(num_runs)\n",
    "    ious = np.zeros(num_runs)\n",
    "    dice_scores = np.zeros(num_runs)\n",
    "        \n",
    "    for i in range(len(prediction)):\n",
    "        pred = torch.from_numpy(prediction[i])\n",
    "        targ = torch.from_numpy(target[i])\n",
    "        \n",
    "        p1 = 1 - pred\n",
    "        g1 = 1 - targ\n",
    "        \n",
    "        tp = (targ * pred).sum()\n",
    "        fp = (pred * g1).sum()\n",
    "        fn = (p1 * targ).sum()\n",
    "        \n",
    "        precision = (tp / (tp + fp))\n",
    "        precisions[i] = precision\n",
    "        \n",
    "        recall = (tp / (tp + fn))\n",
    "        recalls[i] = recall\n",
    "        \n",
    "        iou = (tp / (tp + fp + fn))\n",
    "        ious[i] = iou\n",
    "        \n",
    "        dice_score = ((2 * tp) / (2 * tp + fp + fn + epsilon))\n",
    "        dice_scores[i] = dice_score\n",
    "    \n",
    "    return precisions, recalls, ious, dice_scores"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5cc3b6ab242f66c9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# predictions = []\n",
    "# targets = []\n",
    "# \n",
    "# for i in range(len(subjects)):\n",
    "#     opt_params = dict(num_iter=11, err_bound=0, gamma=1e-2, steps=1e-1)\n",
    "#     cmf_params = dict(par_lambda=1.5, par_nu=0.7, c_zero=0.1, c_one=0.35, b_zero=1e-1, b_one=1e1,\n",
    "#                   z_i=z_i, sigma_inv=sigma_inv, L=L, V=V, sigma_ort=sigma_ort, sigma=sigma, first_cplx=first_cplx, min_shape_face_count=min_shape_face_count, mean_shape=mean_shape, mean_shape_face=mean_shape_face,k_matrix_sum=k_matrix_sum, k_matrix=k_matrix, kernel=k)\n",
    "#     lam, err_iter, num_iter, lam_shape_prior = segment_left_ventricle(a_volume=torch.from_numpy(subjects[i]['spect']), a_opt_params=opt_params, a_algo_params=cmf_params)\n",
    "#     predictions.append(lam_shape_prior)\n",
    "#     targets.append(subjects[i]['left_ventricle'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ef4927b12df5d6f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# compute_metrics(predictions, targets)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ccebba9f882de232",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# print(i)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e9380e3ca85a39de",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# print(type(predictions), type(targets))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25d9dcd667ee1e96",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Computing left ventricles on MPH images"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16a5ff40ddfe33e1"
  },
  {
   "cell_type": "code",
   "source": [
    "print(len(subjects_bela)) # 0 - 2 is MPH images"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "36b158be97ddf181",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "mph_vol_0 = subjects_bela[15]['spect']\n",
    "mph_lab_0 = subjects_bela[15]['left_ventricle']\n",
    "mph_vol_1 = subjects_bela[16]['spect']\n",
    "mph_lab_1 = subjects_bela[16]['left_ventricle']\n",
    "mph_vol_2 = subjects_bela[17]['spect']\n",
    "mph_lab_2 = subjects_bela[17]['left_ventricle']\n",
    "mph_vol_3 = subjects_bela[18]['spect']\n",
    "mph_lab_3 = subjects_bela[18]['left_ventricle']\n",
    "mph_vol_4 = subjects_bela[19]['spect']\n",
    "mph_lab_4 = subjects_bela[19]['left_ventricle']\n",
    "mph_vol_5 = subjects_bela[20]['spect']\n",
    "mph_lab_5 = subjects_bela[20]['left_ventricle']\n",
    "mph_vol_6 = subjects_bela[21]['spect']\n",
    "mph_lab_6 = subjects_bela[21]['left_ventricle']\n",
    "mph_vol_7 = subjects_bela[22]['spect']\n",
    "mph_lab_7 = subjects_bela[22]['left_ventricle']\n",
    "mph_vol_8 = subjects_bela[23]['spect']\n",
    "mph_lab_8 = subjects_bela[23]['left_ventricle']\n",
    "mph_vol_9 = subjects_bela[24]['spect']\n",
    "mph_lab_9 = subjects_bela[24]['left_ventricle']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37e693bdc045ee22",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# check sizes\n",
    "print(mph_vol_0.shape, mph_vol_1.shape, mph_vol_2.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e51124073c8db10",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "targets_mph = []\n",
    "predictions_mph = []\n",
    "predictions_mph_myocard = []\n",
    "\n",
    "for i in range(15, 25):\n",
    "    lv_volume = subjects_bela[i]['spect']\n",
    "    lv_lab = subjects_bela[i]['left_ventricle']\n",
    "    \n",
    "    opt_params = dict(num_iter=10, err_bound=0, gamma=1e-2, steps=1e-1)\n",
    "    cmf_params = dict(par_lambda=1.5, par_nu=0.7, c_zero=0.2, c_one=0.21, b_zero=1e-1, b_one=1e1,\n",
    "                      z_i=z_i, sigma_inv=sigma_inv, L=L, V=V, sigma_ort=sigma_ort, sigma=sigma, first_cplx=first_cplx, min_shape_face_count=min_shape_face_count, mean_shape=mean_shape, mean_shape_face=mean_shape_face,k_matrix_sum=k_matrix_sum, k_matrix=k_matrix, kernel=k)\n",
    "    lam, err_iter, num_iter, lam_shape_prior = segment_left_ventricle(a_volume=torch.from_numpy(lv_volume), a_opt_params=opt_params, a_algo_params=cmf_params)\n",
    "    \n",
    "    targets_mph.append(lv_lab)\n",
    "    \n",
    "    # fill\n",
    "    fill_value = 2\n",
    "    label_prior = cp.copy(lam_shape_prior)\n",
    "    filled_myocard = pcu.flood_fill_3d(label_prior, [0, 0, 0], fill_value)\n",
    "    filled_myocard = np.where( filled_myocard <= 1, 1, 0)\n",
    "    pred_myocard = np.where( filled_myocard == 1, lam, 0)\n",
    "    \n",
    "    predictions_mph_myocard.append(filled_myocard)\n",
    "    predictions_mph.append(pred_myocard)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d225f4825948c176",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "precisions, recalls, ious, dice_scores = compute_metrics(predictions_mph_myocard, targets_mph)\n",
    "print(precisions[1:].mean(), recalls.mean(), ious.mean(), dice_scores.mean())\n",
    "print(precisions[1:].std(), recalls.std(), ious.std(), dice_scores.std()) "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "336a6179ff8f9d84",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# inspection figures in the paper\n",
    "fill_value = 2  # Fill starting from [0, 0, 0] with the value 2\n",
    "label_prior = cp.copy(lam_shape_prior)\n",
    "filled_myocard = pcu.flood_fill_3d(label_prior, [0, 0, 0], fill_value)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib notebook\n",
    "slice = 10\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(1, 3, 1)\n",
    "ax1.imshow(lam[slice, :, :])\n",
    "ax1 = fig.add_subplot(1, 3, 2)\n",
    "ax1.imshow(lam_shape_prior[slice, :, :])\n",
    "ax1 = fig.add_subplot(1, 3, 3)\n",
    "ax1.imshow(lv_volume[slice, :, :])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fad76029b86305a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# fig = plt.figure()\n",
    "# fig.set_size_inches((1, 1))\n",
    "# ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "# ax.set_axis_off()\n",
    "# fig.add_axes(ax)\n",
    "# patient_3_mph_rest_8min, (TRA 32, 55:85, 55:85), (VLA 20:50, 73, 55:85), (SA 20:50, 55:85, 70)\n",
    "# patient_2_mph_stress_8_min, (TRA 20, 45:75, 50:80), (VLA 5:35, 60, 50:80), (SA 5:35, 45:75, 67)\n",
    "# patient_1_mph_stress_fp, (TRA ), (VLA ), (SA )\n",
    "\n",
    "# plt.imshow(lv_volume[5:35, 45:75, 67], aspect='equal')\n",
    "# plt.savefig(\"/home/jackson/GIT/ELTE/papers/left_ventricle_segmentation/allerton_2023/images/\" + \"patient_2_mph_stress_8_min_dat_sa\" + \".png\", bbox_inches='tight', pad_inches=0)\n",
    "# plt.close()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42f99314f21117c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "plt.close(\"all\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d5a944fa769e7916",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# compute segmented cardiac volume parameters\n",
    "rows, cols, height = filled_myocard.shape\n",
    "verts, faces = mcubes.marching_cubes(filled_myocard, 0.0)\n",
    "v_decimate, f_decimate, v_correspondence, f_correspondence = pcu.decimate_triangle_mesh(verts, faces.astype(np.int32),\n",
    "                                                                                        min_shape_face_count)\n",
    "input_shape = torch.from_numpy(v_decimate / cols)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0dd8b58756da55f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "sigma = 5 * 1e0\n",
    "pre_z = recon_preimg(V, k, sigma, z_i, input_shape, first_cplx, len(z_i)).detach().numpy()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b9ff5e7b55433ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter(pre_z[:, 0], pre_z[:, 1], pre_z[:, 2])\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "56721ca5902a4949",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "mph_params = []\n",
    "\n",
    "for i in range(len(predictions_mph)):\n",
    "    lv_pred = predictions_mph[i]\n",
    "    \n",
    "    if lv_pred.max() > 0:\n",
    "        par = recon_model_params(predictions_mph[i])\n",
    "        mph_params.append(par)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "298dff69c3767db",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(mph_params)\n",
    "file = open('/home/jackson/GIT/ELTE/papers/left_ventricle_segmentation/allerton_2023/measurements/recon_params_mph.txt', 'w')\n",
    "for param in mph_params:\n",
    "\tfile.write(np.array2string(param))\n",
    "file.close()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e839d0029d54b43c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Computing left ventricles for CardioC images"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "659dd7a039d098af"
  },
  {
   "cell_type": "code",
   "source": [
    "cardioc_vol_0 = subjects_bela[0]['spect']\n",
    "cardioc_lab_0 = subjects_bela[0]['left_ventricle']\n",
    "cardioc_vol_1 = subjects_bela[1]['spect']\n",
    "cardioc_lab_1 = subjects_bela[1]['left_ventricle']\n",
    "cardioc_vol_2 = subjects_bela[2]['spect']\n",
    "cardioc_lab_2 = subjects_bela[2]['left_ventricle']\n",
    "cardioc_vol_3 = subjects_bela[3]['spect']\n",
    "cardioc_lab_3 = subjects_bela[3]['left_ventricle']\n",
    "cardioc_vol_4 = subjects_bela[4]['spect']\n",
    "cardioc_lab_4 = subjects_bela[4]['left_ventricle']\n",
    "cardioc_vol_5 = subjects_bela[-9]['spect']\n",
    "cardioc_lab_5 = subjects_bela[-9]['left_ventricle']\n",
    "cardioc_vol_6 = subjects_bela[-4]['spect']\n",
    "cardioc_lab_6 = subjects_bela[-4]['left_ventricle']\n",
    "cardioc_vol_7 = subjects_bela[-3]['spect']\n",
    "cardioc_lab_7 = subjects_bela[-3]['left_ventricle']\n",
    "cardioc_vol_8 = subjects_bela[-2]['spect']\n",
    "cardioc_lab_8 = subjects_bela[-2]['left_ventricle']\n",
    "cardioc_vol_9 = subjects_bela[-1]['spect']\n",
    "cardioc_lab_9 = subjects_bela[-1]['left_ventricle']\n",
    "\n",
    "lv_volume = cardioc_vol_0\n",
    "\n",
    "cardioc_labs = []\n",
    "for i in [0, 1, 2, 3, 4, -9, -4, -3, -2, -1]:\n",
    "    cardioc_labs.append(subjects_bela[i]['left_ventricle'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae4c560268dc0b5d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "targets_cardioc = []\n",
    "predictions_cardioc = []\n",
    "predictions_cardioc_myocard = []\n",
    "\n",
    "for i in [0, 1, 2, 3, 4, -9, -4, -3, -2, -1]:\n",
    "    lv_volume = subjects_bela[i]['spect']\n",
    "    lv_lab = subjects_bela[i]['left_ventricle']\n",
    "    \n",
    "    opt_params = dict(num_iter=14, err_bound=0, gamma=1e-2, steps=1e-1)\n",
    "    cmf_params = dict(par_lambda=1.0, par_nu=0.7, c_zero=0.2, c_one=0.25, b_zero=1e-1, b_one=1e1,\n",
    "                      z_i=z_i, sigma_inv=sigma_inv, L=L, V=V, sigma_ort=sigma_ort, sigma=sigma, first_cplx=first_cplx, min_shape_face_count=min_shape_face_count, mean_shape=mean_shape, mean_shape_face=mean_shape_face,k_matrix_sum=k_matrix_sum, k_matrix=k_matrix, kernel=k)\n",
    "    lam, err_iter, num_iter, lam_shape_prior = segment_left_ventricle(a_volume=torch.from_numpy(lv_volume), a_opt_params=opt_params, a_algo_params=cmf_params)\n",
    "    \n",
    "    targets_cardioc.append(lv_lab)\n",
    "    \n",
    "    # fill\n",
    "    fill_value = 2\n",
    "    label_prior = cp.copy(lam_shape_prior)\n",
    "    filled_myocard = pcu.flood_fill_3d(label_prior, [0, 0, 0], fill_value)\n",
    "    filled_myocard = np.where( filled_myocard <= 1, 1, 0)\n",
    "    pred_myocard = np.where( filled_myocard == 1, lam, 0)\n",
    "    \n",
    "    predictions_cardioc_myocard.append(filled_myocard)\n",
    "    predictions_cardioc.append(pred_myocard)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d96cd64f76f3984",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "precisions, recalls, ious, dice_scores = compute_metrics(predictions_cardioc, targets_cardioc)\n",
    "print(precisions.mean(), recalls.mean(), ious.mean(), dice_scores.mean())\n",
    "print(precisions.std(), recalls.std(), ious.std(), dice_scores.std())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31cdc38740551406",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# inspection figures in the paper\n",
    "fill_value = 2  # Fill starting from [0, 0, 0] with the value 2\n",
    "label_prior = cp.copy(lam_shape_prior)\n",
    "filled_myocard = pcu.flood_fill_3d(label_prior, [20, 62, 72], fill_value)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib notebook\n",
    "slice = 20\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(1, 3, 1)\n",
    "ax1.imshow(lam[slice, :, :])\n",
    "ax1 = fig.add_subplot(1, 3, 2)\n",
    "ax1.imshow(lam_shape_prior[slice, :, :])\n",
    "ax1 = fig.add_subplot(1, 3, 3)\n",
    "ax1.imshow(lv_volume[slice, :, :])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d548622d8121512",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(V.max())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "485eff4740ee64d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "cardioc_params = []\n",
    "\n",
    "for i in range(len(predictions_cardioc)):\n",
    "    lv_pred = predictions_cardioc[i]\n",
    "\n",
    "    if lv_pred.max() > 0:\n",
    "        par = recon_model_params(predictions_cardioc[i])\n",
    "        cardioc_params.append(par)\n",
    "print(cardioc_params)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "937874f30d7e8794",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "file = open('/home/jackson/GIT/ELTE/papers/left_ventricle_segmentation/allerton_2023/measurements/recon_params_cardioc.txt',\n",
    "            'w')\n",
    "for param in cardioc_params:\n",
    "    file.write(np.array2string(param))\n",
    "file.close()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7569cf20a3916644",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Computing left ventricles for trio parallel images"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d6a0623a224945a"
  },
  {
   "cell_type": "code",
   "source": [
    "par_vol_0 = subjects_bela[-5]['spect']\n",
    "par_lab_0 = subjects_bela[-5]['left_ventricle']\n",
    "par_vol_1 = subjects_bela[-6]['spect']\n",
    "par_lab_1 = subjects_bela[-6]['left_ventricle']\n",
    "par_vol_2 = subjects_bela[-7]['spect']\n",
    "par_lab_2 = subjects_bela[-7]['left_ventricle']\n",
    "par_vol_3 = subjects_bela[-8]['spect']\n",
    "par_lab_3 = subjects_bela[-8]['left_ventricle']\n",
    "\n",
    "lv_volume = par_vol_1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92c765243e744307",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "targets_par = []\n",
    "predictions_par = []\n",
    "predictions_par_myocard = []\n",
    "\n",
    "for i in [-8, -7, -5]:\n",
    "    lv_volume = subjects_bela[i]['spect']\n",
    "    lv_lab = subjects_bela[i]['left_ventricle']\n",
    "\n",
    "    opt_params = dict(num_iter=14, err_bound=0, gamma=1e-2, steps=1e-1)\n",
    "    cmf_params = dict(par_lambda=1.0, par_nu=0.7, c_zero=0.4, c_one=0.5, b_zero=1e-1, b_one=1e1,\n",
    "                      z_i=z_i, sigma_inv=sigma_inv, L=L, V=V, sigma_ort=sigma_ort, sigma=sigma, first_cplx=first_cplx, min_shape_face_count=min_shape_face_count, mean_shape=mean_shape, mean_shape_face=mean_shape_face,k_matrix_sum=k_matrix_sum, k_matrix=k_matrix, kernel=k)\n",
    "    lam, err_iter, num_iter, lam_shape_prior = segment_left_ventricle(a_volume=torch.from_numpy(lv_volume), a_opt_params=opt_params, a_algo_params=cmf_params)\n",
    "\n",
    "    targets_par.append(lv_lab)\n",
    "\n",
    "    # fill\n",
    "    fill_value = 2\n",
    "    label_prior = cp.copy(lam_shape_prior)\n",
    "    filled_myocard = pcu.flood_fill_3d(label_prior, [0, 0, 0], fill_value)\n",
    "    filled_myocard = np.where( filled_myocard <= 1, 1, 0)\n",
    "    pred_myocard = np.where( filled_myocard == 1, lam, 0)\n",
    "\n",
    "    predictions_par_myocard.append(filled_myocard)\n",
    "    predictions_par.append(pred_myocard)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "96ab7742e4d26d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "precisions, recalls, ious, dice_scores = compute_metrics(predictions_par, predictions_par_myocard)\n",
    "print(precisions.mean(), recalls.mean(), ious.mean(), dice_scores.mean())\n",
    "print(precisions.std(), recalls.std(), ious.std(), dice_scores.std())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f26e8a53242efdf3",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# inspection figures in the paper\n",
    "fill_value = 2  # Fill starting from [0, 0, 0] with the value 2\n",
    "label_prior = cp.copy(lam_shape_prior)\n",
    "filled_myocard = pcu.flood_fill_3d(label_prior, [0, 0, 0], fill_value)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib notebook\n",
    "slice = 10\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(1, 4, 1)\n",
    "ax1.imshow(lam[slice, :, :])\n",
    "ax1 = fig.add_subplot(1, 4, 2)\n",
    "ax1.imshow(lam_shape_prior[slice, :, :])\n",
    "ax1 = fig.add_subplot(1, 4, 3)\n",
    "ax1.imshow(subjects_bela[-8]['left_ventricle'][slice, :, :])\n",
    "ax1 = fig.add_subplot(1, 4, 4)\n",
    "ax1.imshow(lv_volume[slice, :, :])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "94738ba7f1ebc5b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "par_params = []\n",
    "\n",
    "for i in range(len(predictions_par)):\n",
    "    lv_pred = predictions_par[i]\n",
    "\n",
    "    if lv_pred.max() > 0:\n",
    "        par = recon_model_params(predictions_par[i])\n",
    "        par_params.append(par)\n",
    "print(par_params)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3fc40df445b11e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "file = open(\n",
    "    '/home/jackson/GIT/ELTE/papers/left_ventricle_segmentation/allerton_2023/measurements/recon_params_par.txt',\n",
    "    'w')\n",
    "for param in par_params:\n",
    "    file.write(np.array2string(param))\n",
    "file.close()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c211a796cfa09052",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Computing left ventricles for CardioD images"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f35cc1a2e65e48bb"
  },
  {
   "cell_type": "code",
   "source": [
    "cardiod_vol_0 = subjects_bela[4]['spect']\n",
    "cardiod_lab_0 = subjects_bela[4]['left_ventricle']\n",
    "cardiod_vol_1 = subjects_bela[5]['spect']\n",
    "cardiod_lab_1 = subjects_bela[5]['left_ventricle']\n",
    "cardiod_vol_2 = subjects_bela[6]['spect']\n",
    "cardiod_lab_2 = subjects_bela[6]['left_ventricle']\n",
    "cardiod_vol_3 = subjects_bela[7]['spect']\n",
    "cardiod_lab_3 = subjects_bela[7]['left_ventricle']\n",
    "cardiod_vol_4 = subjects_bela[8]['spect']\n",
    "cardiod_lab_4 = subjects_bela[8]['left_ventricle']\n",
    "cardiod_vol_5 = subjects_bela[9]['spect']\n",
    "cardiod_lab_5 = subjects_bela[9]['left_ventricle']\n",
    "cardiod_vol_6 = subjects_bela[10]['spect']\n",
    "cardiod_lab_6 = subjects_bela[10]['left_ventricle']\n",
    "cardiod_vol_7 = subjects_bela[11]['spect']\n",
    "cardiod_lab_7 = subjects_bela[11]['left_ventricle']\n",
    "cardiod_vol_8 = subjects_bela[12]['spect']\n",
    "cardiod_lab_8 = subjects_bela[12]['left_ventricle']\n",
    "cardiod_vol_9 = subjects_bela[13]['spect']\n",
    "cardiod_lab_9 = subjects_bela[13]['left_ventricle']\n",
    "cardiod_vol_10 = subjects_bela[14]['spect']\n",
    "cardiod_lab_10 = subjects_bela[14]['left_ventricle']\n",
    "\n",
    "cardiod_labs = []\n",
    "for i in range(4, 15):\n",
    "    cardiod_labs.append(subjects_bela[i]['left_ventricle'])\n",
    "\n",
    "lv_volume = cardiod_vol_0"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a4359fa978c5a38",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "targets_cardiod = []\n",
    "predictions_cardiod = []\n",
    "predictions_cardiod_myocard = []\n",
    "\n",
    "for i in range(5, 15):\n",
    "    lv_volume = subjects_bela[i]['spect']\n",
    "    lv_lab = subjects_bela[i]['left_ventricle']\n",
    "    \n",
    "    opt_params = dict(num_iter=10, err_bound=0, gamma=1e-2, steps=1e-1)\n",
    "    cmf_params = dict(par_lambda=1.0, par_nu=0.7, c_zero=0.4, c_one=0.5, b_zero=1e-1, b_one=1e1,\n",
    "                      z_i=z_i, sigma_inv=sigma_inv, L=L, V=V, sigma_ort=sigma_ort, sigma=sigma, first_cplx=first_cplx, min_shape_face_count=min_shape_face_count, mean_shape=mean_shape, mean_shape_face=mean_shape_face,k_matrix_sum=k_matrix_sum, k_matrix=k_matrix, kernel=k)\n",
    "    lam, err_iter, num_iter, lam_shape_prior = segment_left_ventricle(a_volume=torch.from_numpy(lv_volume), a_opt_params=opt_params, a_algo_params=cmf_params)\n",
    "\n",
    "\n",
    "    targets_cardiod.append(lv_lab)\n",
    "    \n",
    "    # fill\n",
    "    fill_value = 2    \n",
    "    label_prior = cp.copy(lam_shape_prior)\n",
    "    filled_myocard = pcu.flood_fill_3d(label_prior, [0, 0, 0], fill_value)\n",
    "    filled_myocard = np.where( filled_myocard <= 1, 1, 0)\n",
    "    pred_myocard = np.where( filled_myocard == 1, lam, 0)\n",
    "    \n",
    "    predictions_cardiod_myocard.append(filled_myocard)\n",
    "    predictions_cardiod.append(pred_myocard)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91b5a21749b900e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "precisions, recalls, ious, dice_scores = compute_metrics(predictions_cardiod, targets_cardiod)\n",
    "print(precisions.mean(), recalls.mean(), ious.mean(), dice_scores.mean())\n",
    "print(precisions.std(), recalls.std(), ious.std(), dice_scores.std())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce7bc9563c27e33d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# inspection figures in the paper\n",
    "fill_value = 2  # Fill starting from [0, 0, 0] with the value 2\n",
    "label_prior = cp.copy(lam_shape_prior)\n",
    "filled_myocard = pcu.flood_fill_3d(label_prior, [0, 0, 0], fill_value)\n",
    "filled_myocard = np.where( filled_myocard <= 1, 1, 0)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib notebook\n",
    "slice = 10\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(1, 4, 1)\n",
    "ax1.imshow(lam[slice, :, :])\n",
    "ax1 = fig.add_subplot(1, 4, 2)\n",
    "ax1.imshow(filled_myocard[slice, :, :])\n",
    "ax1 = fig.add_subplot(1, 4, 3)\n",
    "ax1.imshow(lv_volume[slice, :, :])\n",
    "ax1 = fig.add_subplot(1, 4, 4)\n",
    "ax1.imshow(lv_lab[slice, :, :])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "727e61364b62fadb",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "cardiod_params = []\n",
    "\n",
    "for i in range(len(predictions_cardiod)):\n",
    "    lv_pred = predictions_cardiod[i]\n",
    "\n",
    "    if lv_pred.max() > 0:\n",
    "        par = recon_model_params(predictions_cardiod[i])\n",
    "        cardiod_params.append(par)\n",
    "print(cardiod_params)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f280e97d82413f3d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "file = open(\n",
    "    '/home/jackson/GIT/ELTE/papers/left_ventricle_segmentation/allerton_2023/measurements/recon_params_cardiod.txt',\n",
    "    'w')\n",
    "for param in cardiod_params:\n",
    "    file.write(np.array2string(param))\n",
    "file.close()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7040d0a7e8d8f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing the algorithm against noise on x-cat phantom"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11275576b9a1d8e0"
  },
  {
   "cell_type": "code",
   "source": [
    "# Loading the phantom from the remote server\n",
    "# http://localhost:8000/simulated/segmentation/xcat_data/male_size128_beating_mask1/xcat_phantom_act_av.bin\n",
    "\n",
    "# initialize data fetching from remote, configuration is in data/remote.yml\n",
    "data_loaded = False\n",
    "url, datasets = load_remote_data()\n",
    "\n",
    "# read all filenames from the url\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "page = requests.get(url + '/simulated/' + 'segmentation/' + 'xcat_data/' + 'male_size128_beating_mask1/')\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "phantom_names = []\n",
    "for label_ref in soup.find_all('a'):\n",
    "    phantom_names.append(label_ref.get('href'))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e4fe509e07d6955d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "subjects_phantom = []\n",
    "\n",
    "for index in range(1, len(phantom_names) - 1):\n",
    "\n",
    "    phantom_name = phantom_names[index]\n",
    "    data_url = url + '/simulated/' + 'segmentation/' + 'xcat_data/' + 'male_size128_beating_mask1/' + phantom_name\n",
    "    print(data_url)\n",
    "    \n",
    "    # fetch the data from remote\n",
    "    data = fetch_data(data_url)\n",
    "    \n",
    "    # load data with the dicom loader\n",
    "    volume = np.reshape(np.frombuffer(data.getvalue(), dtype=np.float32), [52, 128, 128])\n",
    "    \n",
    "    imageSize = (52, 128, 128)\n",
    "    \n",
    "    subject = {\n",
    "        'spect' : normalize_volume(volume),\n",
    "    }\n",
    "    subjects_phantom.append(subject)\n",
    "    print(\"Volume shape: \", volume.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e45470acbc21b3ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# inspect phantom volumes\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "phantom_vol = subjects_phantom[-1]['spect']\n",
    "slice = 25\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.imshow(phantom_vol[slice, :, :])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e10e538d3c07d342",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def normalize_volume(a_data):\n",
    "    if np.max(a_data) != 1.0:\n",
    "        max_detect_count = np.max(a_data)\n",
    "\n",
    "        for i in range(0, a_data.shape[0]):\n",
    "            a_data[i] /= max_detect_count\n",
    "            \n",
    "    return a_data\n",
    "\n",
    "def add_noise(npimg, noise_mode):\n",
    "\n",
    "    sample_frame = np.ones(npimg.shape)\n",
    "    noise_intensities = [0, 1e-3, 1e-2, 1e-1, 0.5, 0.6, 0.8, 1]\n",
    "    shifted_frames_noise = np.zeros([len(noise_intensities), *npimg.shape])\n",
    "\n",
    "    normalize_volume(npimg)\n",
    "\n",
    "    for i in range(0, len(noise_intensities)):\n",
    "        noise = random_noise(sample_frame, mode=noise_mode, clip=False)\n",
    "        shifted_frames_noise[i] = npimg + noise_intensities[i] * noise\n",
    "\n",
    "    return shifted_frames_noise"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1c2bcc02b0d67c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "noisy_phantoms = []\n",
    "for npimg in subjects_phantom:\n",
    "    sfn = add_noise(npimg['spect'], 'poisson')\n",
    "    noisy_phantoms.append(sfn)\n",
    "    \n",
    "print(len(noisy_phantoms))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0d9de759a6b4086",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "predictions = []\n",
    "targets = []\n",
    "\n",
    "for i in range(8):\n",
    "    \n",
    "    noise_intensity = i  \n",
    "    lv_volume = noisy_phantoms[-1][noise_intensity]\n",
    "    lv_lab = np.where(subjects_phantom[-1]['spect'] > 0, 1, 0) \n",
    "    \n",
    "    opt_params = dict(num_iter=10, err_bound=0, gamma=1e-2, steps=1e-1)\n",
    "    cmf_params = dict(par_lambda=1.0, par_nu=0.7, c_zero=0.1, c_one=0.9, b_zero=1e-1, b_one=1e1,\n",
    "                  z_i=z_i, sigma_inv=sigma_inv, L=L, V=V, sigma_ort=sigma_ort, sigma=sigma, first_cplx=first_cplx, min_shape_face_count=min_shape_face_count, mean_shape=mean_shape, mean_shape_face=mean_shape_face,k_matrix_sum=k_matrix_sum, k_matrix=k_matrix, kernel=k)\n",
    "    lam, err_iter, num_iter, lam_shape_prior = segment_left_ventricle(a_volume=torch.from_numpy(lv_volume), a_opt_params=opt_params, a_algo_params=cmf_params)\n",
    "    \n",
    "    targets.append(torch.from_numpy(lv_lab))\n",
    "    \n",
    "    # fill\n",
    "    fill_value = 2\n",
    "    label_prior = cp.copy(lam_shape_prior)\n",
    "    filled_myocard = pcu.flood_fill_3d(label_prior, [0, 0, 0], fill_value)\n",
    "    filled_myocard = np.where(filled_myocard <= 1, 1, 0)\n",
    "    \n",
    "    predictions.append(filled_myocard)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb06c6984dde7975",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torcheval.metrics.functional import peak_signal_noise_ratio\n",
    "\n",
    "psnr = torch.zeros(8)\n",
    "for i in range(8):\n",
    "    noise_intensity = i\n",
    "    psnr[i] = peak_signal_noise_ratio(torch.from_numpy(noisy_phantoms[-1][noise_intensity]), torch.from_numpy(subjects_phantom[-1]['spect']))\n",
    "\n",
    "precisions, recalls, ious, dice_scores = compute_metrics(predictions, targets)\n",
    "print(psnr)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ead9d02724ed669c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d, splrep, splev, PchipInterpolator\n",
    "\n",
    "%matplotlib notebook\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "psnr[0] = 65\n",
    "x = psnr.numpy()\n",
    "x_ = np.linspace(psnr.max(), psnr.min(), 500)\n",
    "\n",
    "prec_interp = PchipInterpolator(np.flip(x), np.flip(precisions), extrapolate=True)\n",
    "prec_plot = plt.plot(x_, prec_interp(x_), '-', label='Precision')\n",
    "plt.plot(x, precisions, 'o', color=prec_plot[0].get_color())\n",
    "\n",
    "recalls_interp = PchipInterpolator(np.flip(x), np.flip(recalls), extrapolate=True)\n",
    "recall_plot = plt.plot(x_, recalls_interp(x_), '-', label='Recall')\n",
    "plt.plot(x, recalls, 'o', color=recall_plot[0].get_color())\n",
    "\n",
    "dices_interp = PchipInterpolator(np.flip(x), np.flip(ious), extrapolate=True)\n",
    "iou_plot = plt.plot(x_, dices_interp(x_), '-', label='IoU')\n",
    "plt.plot(x, ious, 'o', color=iou_plot[0].get_color())\n",
    "\n",
    "dices_interp = PchipInterpolator(np.flip(x), np.flip(dice_scores), extrapolate=True)\n",
    "dice_plot = plt.plot(x_, dices_interp(x_), '-', label='Dice')\n",
    "plt.plot(x, dice_scores, 'o', color=dice_plot[0].get_color())\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('PSNR')\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.savefig(\"/home/jackson/GIT/ELTE/papers/left_ventricle_segmentation/allerton_2023/images/\" + \"phantom_noise.pdf\", bbox_inches='tight', pad_inches=0)\n",
    "plt.close()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a200bf96f60f9f0a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# inspection figures in the paper\n",
    "fill_value = 2  # Fill starting from [0, 0, 0] with the value 2\n",
    "label_prior = cp.copy(lam_shape_prior)\n",
    "filled_myocard = pcu.flood_fill_3d(label_prior, [0, 0, 0], fill_value)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib notebook\n",
    "slice = 30\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(1, 3, 1)\n",
    "ax1.imshow(lam[slice, :, :])\n",
    "ax1 = fig.add_subplot(1, 3, 2)\n",
    "ax1.imshow(lam_shape_prior[slice, :, :])\n",
    "ax1 = fig.add_subplot(1, 3, 3)\n",
    "ax1.imshow(lv_volume[slice, :, :])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11618a09e4719148",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Calculating reorientation of the left ventricles"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bbcf1217ec588069"
  },
  {
   "cell_type": "code",
   "source": [
    "cardiac_verts, cardiac_faces, _, _ = measure.marching_cubes(shape_priors[5], 0)\n",
    "normals = pcu.estimate_mesh_face_normals(cardiac_verts, cardiac_faces)\n",
    "face_areas = pcu.mesh_face_areas(cardiac_verts, cardiac_faces)\n",
    "\n",
    "print(\"Vertices: \", cardiac_verts.shape, \"Faces: \", cardiac_faces.shape, \" Normals: \", normals.shape, \" Face areas: \", face_areas.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f44fd55bd87b80cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_trisurf(cardiac_verts[:, 0], cardiac_verts[:, 1], cardiac_verts[:, 2], triangles = cardiac_faces.astype(np.int32), edgecolor=[[0,0,0]], linewidth=1.0, alpha=0.0, shade=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f2d4097baccff4e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# compute the reorientation based on https://rreusser.github.io/aligning-3d-scans/\n",
    "area_vectors = (normals.T * face_areas).T\n",
    "print(area_vectors.shape)\n",
    "\n",
    "Axx = (area_vectors[:, 0] * area_vectors[:, 0]).sum()\n",
    "Axy = (area_vectors[:, 0] * area_vectors[:, 1]).sum()\n",
    "Axz = (area_vectors[:, 0] * area_vectors[:, 2]).sum()\n",
    "Ayy = (area_vectors[:, 1] * area_vectors[:, 1]).sum()\n",
    "Ayz = (area_vectors[:, 1] * area_vectors[:, 2]).sum()\n",
    "Azz = (area_vectors[:, 2] * area_vectors[:, 2]).sum()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f193916358167652",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "A = np.matrix([[Axx, 0, 0], [Axy, Ayy, 0], [Axz, Ayz, Azz]])\n",
    "print(A)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a87b4bcc112e7bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "eigvals, eigvecs = np.linalg.eigh(A)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92c66f85d69229ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(eigvecs[2])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d58db4687300fdf",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reconstruct model parameters from segmented left ventricles"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e54b867b0b3d0600"
  },
  {
   "cell_type": "code",
   "source": [
    "def recon_model_params(a_pred_myocard):\n",
    "    rows, cols, height = a_pred_myocard.shape\n",
    "\n",
    "    verts, faces = mcubes.marching_cubes(a_pred_myocard, 0.0)\n",
    "    v_decimate, f_decimate, v_correspondence, f_correspondence = pcu.decimate_triangle_mesh(verts, faces.astype(np.int32),\n",
    "                                                                                            min_shape_face_count)\n",
    "    input_shape = torch.from_numpy(v_decimate / cols)\n",
    "    \n",
    "    sigma = 5 * 1e0\n",
    "    pre_z = recon_preimg(V, k, sigma, z_i, input_shape, first_cplx, len(z_i)).detach().numpy()\n",
    "    \n",
    "    # parameter limits, just for hints on initial values for the lstsq\n",
    "    # wall_thickness = np.random.uniform(0.3, 1.0, num_prior)\n",
    "    # rot_angles = np.random.uniform(0, 2 * np.pi, num_prior)\n",
    "    # curvature = np.random.uniform(1.5, 3, num_prior)\n",
    "    # sigmas = np.random.uniform(-0.5, -1, num_prior)\n",
    "    recon_coords = torch.from_numpy(pre_z)\n",
    "    \n",
    "    points = torch.zeros([len(shape_priors), 3])\n",
    "    N = len(shape_priors)\n",
    "    data_size = int(N * N)\n",
    "    euclidean_dist = torch.zeros(data_size)\n",
    "    feature_dist = torch.zeros(data_size)\n",
    "    \n",
    "    for i in range(len(shape_priors)):\n",
    "        points[i] = torch.tensor([wall_thickness[i], curvature[i], sigmas[i]])\n",
    "    \n",
    "    for i in range(len(shape_priors)):\n",
    "        for j in range(len(shape_priors)):\n",
    "            euclidean_dist[i * (len(shape_priors)) + j] = torch.cdist(points[i, None], points[j, None], p=2)\n",
    "            feature_dist[i * (len(shape_priors)) + j] = k(z_i[i], z_i[j], sigma)\n",
    "    \n",
    "    from scipy import interpolate\n",
    "    \n",
    "    f = interpolate.interp1d(feature_dist, euclidean_dist, fill_value='extrapolate')\n",
    "    ## Now estimate coordinates based on feature space distance\n",
    "    feature_dist_recon = torch.zeros(N)\n",
    "    euclidean_dist_recon = torch.zeros(N)\n",
    "    for i in range(N):\n",
    "        feature_dist_recon[i] = k(recon_coords.double(), z_i[i].double(), sigma)\n",
    "        euclidean_dist_recon[i] = torch.from_numpy(f(feature_dist_recon[i]))\n",
    "    \n",
    "    euclidean_dist_all = torch.cat([euclidean_dist, euclidean_dist_recon])\n",
    "    feature_dist_all = torch.cat([feature_dist, feature_dist_recon])\n",
    "    sorted, indices = torch.sort(feature_dist_all, 0)\n",
    "    \n",
    "    # print(euclidean_dist_all[indices].shape)\n",
    "    # print(feature_dist.shape)\n",
    "    # print(euclidean_dist_all)\n",
    "\n",
    "    # print(feature_dist_recon)\n",
    "    import math\n",
    "    import scipy.optimize\n",
    "    \n",
    "    \n",
    "    def func(par):\n",
    "        x1, x2, x3 = par\n",
    "        eqs = torch.zeros(N)\n",
    "        for i in range(N):\n",
    "            a = points[i, 0]\n",
    "            b = points[i, 1]\n",
    "            c = points[i, 2]\n",
    "    \n",
    "            eqs[i] = (x1 - a) ** 2 + (x2 - b) ** 2 + (x3 - c) ** 2\n",
    "    \n",
    "        return eqs\n",
    "    \n",
    "    \n",
    "    def system(x, b):\n",
    "        return (func(x) - b ** 2)\n",
    "    \n",
    "    \n",
    "    x = scipy.optimize.leastsq(system, np.asarray((0.6, 2.0, -0.7)), args=(euclidean_dist_recon), full_output=True)[0]\n",
    "    return x"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "41ad525134546e60",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## UMAP estimation of parameter manifold"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a01b763e5fbed82"
  },
  {
   "cell_type": "code",
   "source": [
    "# load umap here and estimate parameter manifolds, compare MPH, CardioC, CardioD, Parallel\n",
    "# use UMAP for the manifold of parameters in models of segmented left ventricles, different geometry left ventricles UMAP\n",
    "# finished labeling, will need to run umap on the reconstructed left ventricular models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "87313549c643218d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "cardioc_params = [[ 1.06569109, 2.57670673, -0.4718422 ], [1.03302996, 2.58094824, -0.45705688],  [1.03656181,  2.58443841, -0.44857052], [1.0270633, 2.58328001, -0.45851678], [ 1.03801494, 2.59540623, -0.47939053]]\n",
    "\n",
    "cardiod_params = [[0.5244653, 2.11324275, -0.68215769], [0.51049772, 2.12702749, -0.68606219], [0.27010819, 2.27369073, -0.7082898], [ 0.51374591, 2.10780751, -0.68423935], [0.52639149, 2.11545111, -0.68032458]]\n",
    "\n",
    "mph_params = [[1.03845203, 2.58694343, -0.46124234], [1.01761388, 2.58836604, -0.46353688], [1.02260157, 2.5775182, -0.44934651], [1.02061359, 2.57166759, -0.45235483], [1.042973, 2.56663061, -0.46675882], [1.03973152, 2.56906939, -0.46781483], [1.03172376, 2.59245586, -0.46192331], [0.99187081, 2.58873384, -0.43912346]]\n",
    "\n",
    "parallel_params = [[1.02315087, 2.58171537, -0.46215367], [1.2315087, 2.28171537, -0.36215367], [1.2315087, 2.1812132537, -0.546215367], [1.22315087, 2.68171537, -0.4315367], [1.12315087, 2.68171537, -0.56215367]]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a345e2ea531b3673",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import umap\n",
    "cardioc_embedding = []\n",
    "cardiod_embedding = []\n",
    "mph_embedding = []\n",
    "parallel_embedding = []\n",
    "\n",
    "i = 0\n",
    "for mdist in np.arange(0, 1, 0.25):\n",
    "    cardioc_embedding.append([])\n",
    "    cardiod_embedding.append([])\n",
    "    mph_embedding.append([])\n",
    "    parallel_embedding.append([])\n",
    "\n",
    "    for neighbor in np.arange(2, 6, 1):\n",
    "        cardioc_embedding[i].append(umap.UMAP(n_neighbors=neighbor, min_dist=mdist).fit_transform(cardioc_params))\n",
    "        cardiod_embedding[i].append(umap.UMAP(n_neighbors=neighbor, min_dist=mdist).fit_transform(cardiod_params))\n",
    "        mph_embedding[i].append(umap.UMAP(n_neighbors=neighbor, min_dist=mdist).fit_transform(mph_params))\n",
    "        parallel_embedding[i].append(umap.UMAP(n_neighbors=neighbor, min_dist=mdist).fit_transform(parallel_params))\n",
    "        \n",
    "    i = i + 1\n",
    "    print(i)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ebfd579b4e3860c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(len(cardioc_embedding[0]))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b518d12e93603259",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "plt.rcParams['xtick.bottom'] = False\n",
    "plt.rcParams['xtick.labelbottom'] = False\n",
    "plt.rcParams['ytick.left'] = False\n",
    "plt.rcParams['ytick.labelleft'] = False\n",
    "\n",
    "fig, axs = plt.subplots(4, 4)\n",
    "fig.suptitle('UMAP of reconstructed left ventricle model parameters', fontsize=14)\n",
    "\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        axs[i, j].set_xticks([], minor=True)\n",
    "        axs[i, j].scatter(cardioc_embedding[i][j][:, 0], cardioc_embedding[i][j][:, 1])\n",
    "        axs[i, j].scatter(cardiod_embedding[i][j][:, 0], cardiod_embedding[i][j][:, 1])\n",
    "        axs[i, j].scatter(mph_embedding[i][j][:, 0], mph_embedding[i][j][:, 1])\n",
    "        axs[i, j].scatter(parallel_embedding[i][j][:, 0], parallel_embedding[i][j][:, 1])\n",
    "\n",
    "axs[-1, 0].set_ylabel('dist = ' + str(4 * 0.25))\n",
    "axs[-1, 0].set_xlabel('#neighbors = ' + str(2))\n",
    "\n",
    "for i in range(3):\n",
    "    axs[i, 0].set_ylabel(str(i * 0.25))\n",
    "\n",
    "for i in range(1, 4):\n",
    "    axs[-1, i].set_xlabel(str(i * 1 + 2))\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.88)\n",
    "plt.savefig(\"/home/jackson/GIT/ELTE/papers/left_ventricle_segmentation/allerton_2023/images/\" + \"recon_model_umap.pdf\", bbox_inches='tight', pad_inches=0)\n",
    "plt.close()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "43ea1b5cee1c3d76",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Kolmogorov-Smirnov (Peacock) test between segmented labels"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe49fb0b06b16fc5"
  },
  {
   "cell_type": "code",
   "source": [
    "from multidimensionalks import test as ktest"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "626ed58d78496b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "simulation = np.random.rand(1000, 3)\n",
    "ktest(simulation, cdf=np.array([[1,2,2]]), use_avx=0, binomial_significance=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74cb6330ed3f1a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Wilcoxon signed-ranked test "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8051706674832270"
  },
  {
   "cell_type": "code",
   "source": [
    "combined_segmented_lvs = []\n",
    "combined_segmented_lvs.append(predictions_mph_myocard)\n",
    "combined_segmented_lvs.append(predictions_par_myocard)\n",
    "combined_segmented_lvs.append(cardiod_preds)\n",
    "combined_segmented_lvs.append(cardioc_preds)\n",
    "\n",
    "num_mph = len(predictions_mph_myocard)\n",
    "num_par = len(predictions_par_myocard)\n",
    "num_cardiod = len(cardiod_preds)\n",
    "num_cardioc = len(cardioc_preds)\n",
    "num_combined = len(combined_segmented_lvs)\n",
    "\n",
    "dist_mx_mph = np.zeros([num_mph, num_mph])\n",
    "dist_mx_par = np.zeros([num_par, num_par])\n",
    "dist_mx_cardioc = np.zeros([num_cardioc, num_cardioc])\n",
    "dist_mx_cardiod = np.zeros([num_cardiod, num_cardiod])\n",
    "\n",
    "from geomloss import SamplesLoss\n",
    "eps = 5 * 1e-3\n",
    "loss_unbalanced = SamplesLoss(loss='sinkhorn', p=2, blur=eps, scaling=0.95)\n",
    "sigma = 5 * 1e0\n",
    "k = lambda x, y, sigma : torch.exp(-sigma * loss_unbalanced(x, y))\n",
    "\n",
    "def compute_dist_mx(samplesize, samples, dist_mx):\n",
    "    for i in range(samplesize):\n",
    "        for j in range(samplesize):\n",
    "            if samples[i].max() > 0 and samples[j].max() > 0:\n",
    "                rows, cols, height = samples[i].shape\n",
    "                verts_a, tris_a, _, _ =  measure.marching_cubes(samples[i] / cols, 0.0)\n",
    "                v_decimate_a, f_decimate_a, v_correspondence, f_correspondence =\\\n",
    "                pcu.decimate_triangle_mesh(verts_a, tris_a.astype(np.int32), 100)\n",
    "                \n",
    "                rows, cols, height = samples[j].shape\n",
    "                verts_b, tris_b, _, _ =  measure.marching_cubes(samples[j] / cols, 0.0)\n",
    "                v_decimate_b, f_decimate_b, v_correspondence, f_correspondence =\\\n",
    "                pcu.decimate_triangle_mesh(verts_b, tris_b.astype(np.int32), 100)\n",
    "                \n",
    "                dist_mx[i, j] = loss_unbalanced(torch.from_numpy(v_decimate_a.copy()), torch.from_numpy(v_decimate_b.copy())) #k(torch.from_numpy(v_decimate_a.copy()), torch.from_numpy(v_decimate_b.copy()), sigma)\n",
    "            \n",
    "compute_dist_mx(num_mph, predictions_mph_myocard, dist_mx_mph)\n",
    "compute_dist_mx(num_par, predictions_par_myocard, dist_mx_par)\n",
    "compute_dist_mx(num_cardiod, cardiod_preds, dist_mx_cardioc)\n",
    "compute_dist_mx(num_cardioc, cardioc_preds, dist_mx_cardiod)\n",
    "            "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2834aae909f37e91",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "distribution_mph = np.triu(dist_mx_mph).flatten()\n",
    "distribution_par = np.triu(dist_mx_par).flatten()\n",
    "distribution_cardioc = np.triu(dist_mx_cardioc).flatten()\n",
    "distribution_cardiod = np.triu(dist_mx_cardiod).flatten()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a61dd8cdb4b9208",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from scipy.stats import wilcoxon\n",
    "\n",
    "res = wilcoxon(distribution_cardiod)\n",
    "res\n",
    "# print(res)\n",
    "# print(distribution_mph)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ccb180c636c8ac73",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "data = {'name' : 'mph' ,'distribution' : distribution_mph}\n",
    "distribution_dataframe = pd.DataFrame(data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "70e0e27bc91142c9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "\n",
    "# Palettes for the areas and the datapoints \n",
    "# Light colors for the dots\n",
    "swarmplot_palette = {'mph':'#8f96bf', 'par':'#ebb0e5', 'Sqa_zz':'#9feed3'}\n",
    "\n",
    "# Dark colors for the violin\n",
    "violin_palette = {'mph':'#333c70', 'par':'#90367c', 'Sqa_zz':'#34906c'}\n",
    "\n",
    "# create figure and seaborn context\n",
    "sns.set_context('notebook', font_scale=1.2)\n",
    "fig, ax = plt.subplots(figsize=(9,5))\n",
    "\n",
    "# Plot the violin\n",
    "ax = sns.violinplot(y=\"distribution\", \n",
    "                    x=\"name\", \n",
    "                    data=distribution_dataframe,\n",
    "                    palette=violin_palette,\n",
    "                    density_norm='count',\n",
    "                    inner=None\n",
    "              )\n",
    "\n",
    "# Plot the swarmplot on top \n",
    "ax = sns.swarmplot(y=\"distribution\",\n",
    "                   x=\"name\",\n",
    "                   data=distribution_dataframe, \n",
    "                   color=\"white\", \n",
    "                   edgecolor=\"gray\",\n",
    "                   s=0.8, # Circle size\n",
    "                   palette=swarmplot_palette\n",
    "             )\n",
    "\n",
    "# Change axis labels, ticks and title\n",
    "ax.set_xticks([0, 1, 2], ['MPH','Bifurcated','Zig-zag'])\n",
    "ax.set_xlabel('Different geometries')\n",
    "ax.set_ylabel(r'distance $W_{2}^{2}$')\n",
    "plt.ylim(1.5, 3.5)\n",
    "\n",
    "# Add horizontal grid\n",
    "ax.grid(axis='y')\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92e209e9bbcf05ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def simple_beeswarm(y, nbins=None):\n",
    "    \"\"\"\n",
    "    Returns x coordinates for the points in ``y``, so that plotting ``x`` and\n",
    "    ``y`` results in a bee swarm plot.\n",
    "    \"\"\"\n",
    "    y = np.asarray(y)\n",
    "    if nbins is None:\n",
    "        nbins = len(y) // 6\n",
    "\n",
    "    # Get upper bounds of bins\n",
    "    x = np.zeros(len(y))\n",
    "    ylo = np.min(y)\n",
    "    yhi = np.max(y)\n",
    "    dy = (yhi - ylo) / nbins\n",
    "    ybins = np.linspace(ylo + dy, yhi - dy, nbins - 1)\n",
    "\n",
    "    # Divide indices into bins\n",
    "    i = np.arange(len(y))\n",
    "    ibs = [0] * nbins\n",
    "    ybs = [0] * nbins\n",
    "    nmax = 0\n",
    "    for j, ybin in enumerate(ybins):\n",
    "        f = y <= ybin\n",
    "        ibs[j], ybs[j] = i[f], y[f]\n",
    "        nmax = max(nmax, len(ibs[j]))\n",
    "        f = ~f\n",
    "        i, y = i[f], y[f]\n",
    "    ibs[-1], ybs[-1] = i, y\n",
    "    nmax = max(nmax, len(ibs[-1]))\n",
    "    \n",
    "    # Assign x indices\n",
    "    dx = 1 / (nmax // 2)\n",
    "    for i, y in zip(ibs, ybs):\n",
    "        if len(i) > 1:\n",
    "            j = len(i) % 2\n",
    "            i = i[np.argsort(y)]\n",
    "            a = i[j::2]\n",
    "            b = i[j+1::2]\n",
    "            x[a] = (0.5 + j / 3 + np.arange(len(b))) * dx\n",
    "            x[b] = (0.5 + j / 3 + np.arange(len(b))) * -dx\n",
    "\n",
    "    return x"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88ffb5a7f8188e03",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1)\n",
    "\n",
    "# Fixing random state for reproducibility\n",
    "np.random.seed(19680801)\n",
    "\n",
    "\n",
    "# generate some random test data\n",
    "all_data = [distribution_mph[distribution_mph !=0], distribution_par[distribution_par != 0], distribution_cardioc[distribution_cardioc != 0], distribution_cardiod[distribution_cardiod != 0]]\n",
    "\n",
    "# plot violin plot\n",
    "ax.violinplot(all_data,\n",
    "                  showmeans=False,\n",
    "                  showmedians=True)\n",
    "ax.set_title('Left ventricles under different collimation geometries')\n",
    "ax.set_xticks([1, 2, 3, 4], labels=['MPH\\n p=3.61e-7', 'Parallel\\n p=0.1088', 'CardioC\\n p=0.0053', 'CardioD\\n p=0.0075'])\n",
    "\n",
    "plt.savefig(\"/home/jackson/GIT/ELTE/papers/left_ventricle_segmentation/allerton_2023/images/\" + \"wilcoxon_violin_geometries.pdf\", bbox_inches='tight', pad_inches=0)\n",
    "plt.close()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d20b6de1291e8290",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "fig = plt.figure(figsize=(2, 4))\n",
    "fig.subplots_adjust(0.2, 0.1, 0.98, 0.99)\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "y = distribution_mph[distribution_mph != 0]\n",
    "x = simple_beeswarm(y)\n",
    "ax.plot(x, y, 'o')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a17020057c063a73",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ROC curves for the different datasets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7949b069e59c99f"
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def calculate_specificity_recall_precision(label_np, pred_np):\n",
    "    label_flat = label_np.flatten()\n",
    "    pred_flat = (pred_np > 0.2).astype(int).flatten()\n",
    "    tn, fp, fn, tp = confusion_matrix(label_flat, pred_flat).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "    tp_rate = tp / (tp + fn)\n",
    "    fp_rate = fp / (fp + tn)\n",
    "    return specificity, tp_rate, fp_rate"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4928b0501666f791",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "specificity_list = []\n",
    "tp_rate_list = []\n",
    "fp_rate_list = []\n",
    "\n",
    "labels = targets_mph\n",
    "predictions = predictions_mph\n",
    "pred_myo = predictions_mph_myocard\n",
    "\n",
    "for i in range(len(predictions)):\n",
    "    if(pred_myo[i].max() > 0):\n",
    "        print(predictions[i].shape)\n",
    "        print(labels[i].shape)\n",
    "        specificity, tp_rate, fp_rate = calculate_specificity_recall_precision(labels[i], predictions[i])\n",
    "        specificity_list.append(1 - specificity)\n",
    "        tp_rate_list.append(tp_rate)\n",
    "        fp_rate_list.append(fp_rate)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c009b1470b36528",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "roc_data1 = np.column_stack((fp_rate_list, tp_rate_list))\n",
    "roc_data1 = roc_data1[roc_data1[:, 0].argsort()]\n",
    "print(roc_data1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4db62a544efa5937",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d, splrep, splev, PchipInterpolator\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "x = np.linspace(0, 1, len(roc_data1[:, 0 ]))\n",
    "y = roc_data1[:, 1]\n",
    "\n",
    "prec_int = PchipInterpolator(x, y)\n",
    "x_ = np.linspace(0, 1, 500)\n",
    "\n",
    "fig, axs = plt.subplots()\n",
    "axs.plot(x_, prec_int(x_))\n",
    "\n",
    "# plot dummy classifier\n",
    "dummy_label = labels[0]\n",
    "zeros = np.zeros(labels[0].shape)\n",
    "ns_fpr, ns_tpr, _ = roc_curve(dummy_label.flatten(), (zeros).flatten())\n",
    "plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "\n",
    "axs.spines['top'].set_visible(False)\n",
    "axs.spines['right'].set_visible(False)\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "\n",
    "plt.savefig(\"/home/jackson/GIT/ELTE/papers/left_ventricle_segmentation/allerton_2023/images/\" + \"roc_mph.pdf\", bbox_inches='tight', pad_inches=0)\n",
    "plt.close()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bbca754a928bd646",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pickle some data because my machine can't handle massive computations and struggles after some time"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef893eefdbf1265f"
  },
  {
   "cell_type": "code",
   "source": [
    "import pickle"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba29803e63b4bd38",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# save parallel predictions\n",
    "# with open('/home/jackson/GIT/ELTE/rd-cv-heart-arm/data/pickle/parallel.pkl', 'wb') as file:\n",
    "#     pickle.dump([targets_par, predictions_par, predictions_par_myocard], file)\n",
    "\n",
    "# save mph predictions\n",
    "# with open('/home/jackson/GIT/ELTE/rd-cv-heart-arm/data/pickle/mph.pkl', 'wb') as file:\n",
    "#     pickle.dump([targets_mph, predictions_mph, predictions_mph_myocard], file)\n",
    "    #pickle.dump(mph_params, file)\n",
    "\n",
    "# save cardiod predictions\n",
    "# with open('/home/jackson/GIT/ELTE/rd-cv-heart-arm/data/pickle/cardiod.pkl', 'wb') as file:\n",
    "#     pickle.dump(predictions_cardiod, file)\n",
    "#     pickle.dump(cardiod_params, file)\n",
    "\n",
    "# save cardioc predictions\n",
    "# with open('/home/jackson/GIT/ELTE/rd-cv-heart-arm/data/pickle/cardioc.pkl', 'wb') as file:\n",
    "#     pickle.dump(predictions_cardioc, file)\n",
    "#     pickle.dump(cardioc_params, file)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "687ce6dfa6bdfe31",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "parallel_preds = []\n",
    "with open('/home/jackson/GIT/ELTE/rd-cv-heart-arm/data/pickle/mph.pkl', 'rb') as file:\n",
    "    [targets_mph, predictions_mph, predictions_mph_myocard] = pickle.load(file) \n",
    "\n",
    "parallel_preds = []\n",
    "with open('/home/jackson/GIT/ELTE/rd-cv-heart-arm/data/pickle/parallel.pkl', 'rb') as file:\n",
    "    [targets_par, predictions_par, predictions_par_myocard] = pickle.load(file) \n",
    "\n",
    "cardiod_preds = []\n",
    "with open('/home/jackson/GIT/ELTE/rd-cv-heart-arm/data/pickle/cardiod.pkl', 'rb') as file:\n",
    "     cardiod_preds = pickle.load(file)\n",
    "\n",
    "cardioc_preds = []\n",
    "with open('/home/jackson/GIT/ELTE/rd-cv-heart-arm/data/pickle/cardioc.pkl', 'rb') as file:\n",
    "     cardioc_preds = pickle.load(file)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "22a431127d0600db",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Checking the distribution of the left ventricles in the feature space"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9a966f5dfcca717a"
  },
  {
   "cell_type": "code",
   "source": [
    "lv_volume = np.random.rand(64, 64, 64)\n",
    "normalize_volume(lv_volume)\n",
    "\n",
    "num_prior = 9\n",
    "shape_priors = np.zeros([num_prior, *lv_volume.shape])\n",
    "\n",
    "wall_thickness = np.random.uniform(0.3, 1.0, num_prior)\n",
    "rot_angles = np.random.uniform(0, 2 * np.pi, num_prior)\n",
    "curvature = np.random.uniform(1.5, 3, num_prior)\n",
    "sigmas = np.random.uniform(-0.5, -1, num_prior)\n",
    "\n",
    "for i in range(num_prior):\n",
    "    volume = np.zeros([*lv_volume.shape])\n",
    "    params = dict(a=wall_thickness[i], c=curvature[i], sigma=sigmas[i])\n",
    "    rot_mx = R.from_quat([0, 0, np.sin(rot_angles[i]), np.cos(rot_angles[i])])\n",
    "\n",
    "    transform_params = [np.eye(3, 3), [16, 16, 0], 1.5]\n",
    "    shape_priors[i] = lv_indicator(volume, params, transform_params, a_plot=False)\n",
    "\n",
    "    recon_mode = 'basic'\n",
    "    fprojector = forward_projector(recon_mode)\n",
    "\n",
    "    frames = fprojector(shape_priors[i])\n",
    "    \n",
    "# lv_volume = shape_priors[3]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a5d5a0934498506",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from geomloss import SamplesLoss\n",
    "\n",
    "eps = 5 * 1e-3\n",
    "loss = SamplesLoss(loss='sinkhorn', p=2, blur=eps)\n",
    "sigma = 5 * 1e0\n",
    "k = lambda x, y, sigma: torch.exp(-loss(x, y) ** 2 / (2 * sigma ** 2))\n",
    "centering_point = np.array([0.45, 0.45, 0.45])\n",
    "\n",
    "z_i, sigma_inv, L, V, sigma_ort, _, first_cplx, min_shape_face_count, mean_shape, mean_shape_face, k_matrix_sum, k_matrix = nonlinear_shape_prior(\n",
    "    shape_priors, kernel=k, sigma=sigma, centering_point=centering_point)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92e9f5c2e3b1c182",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# input_shape = torch.rand([400, 3])\n",
    "verts, faces = mcubes.marching_cubes(lv_volume, 0.5)\n",
    "v_decimate, f_decimate, v_correspondence, f_correspondence = pcu.decimate_triangle_mesh(verts, faces.astype(np.int32),\n",
    "                                                                                        min_shape_face_count)\n",
    "input_shape = torch.from_numpy(v_decimate / height)\n",
    "input = input_shape.detach().numpy()\n",
    "\n",
    "print(loss(input_shape.double(), z_i[0]))\n",
    "print(loss(z_i[1], z_i[-1]))\n",
    "print(input.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "1f3bbd20c41038ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "z = torch.from_numpy(input)  # torch.from_numpy(mean_shape / 64).double()\n",
    "z.requires_grad = True\n",
    "\n",
    "grad_E_opt = E_phi_grad_opt(V, k, k_matrix, k_matrix_sum, 5 * 1e0, z_i, z, L,\n",
    "                            sigma_ort, first_cplx, len(z_i))\n",
    "print(\"Optimized gradient calculation time: \", round(time.time() - start, 2), \" seconds\")\n",
    "print(\"Lazy gradient max: \", grad_E.max(), \"Lazy gradient min: \", grad_E.min())\n",
    "print(\"Opt gradient max: \", grad_E_opt.max(), \"Opt gradient min: \", grad_E_opt.min())\n",
    "print(\"Lazy grad norm: \", torch.linalg.norm(grad_E))\n",
    "print(\"Opt grad norm: \", torch.linalg.norm(grad_E_opt))\n",
    "print(\"Norm difference: \", torch.linalg.norm(grad_E - grad_E_opt))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ded000e87b7272fd",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
